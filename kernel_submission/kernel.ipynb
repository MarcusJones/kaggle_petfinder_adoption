{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Jun 10 10:32:09 2018\n",
    "\n",
    "@author: m.jones\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Globals"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# LANDSCAPE_A3 = (16.53, 11.69)\n",
    "# PORTRAIT_A3 = (11.69, 16.53)\n",
    "# LANDSCAPE_A4 = (11.69, 8.27)\n",
    "if 'KAGGLE_WORKING_DIR' in os.environ:\n",
    "    DEPLOYMENT = 'Kaggle'\n",
    "else:\n",
    "    DEPLOYMENT = 'Local'\n",
    "\n",
    "if DEPLOYMENT=='Kaggle':\n",
    "    PATH_DATA_ROOT = r\"~\"\n",
    "if DEPLOYMENT == 'Local':\n",
    "    PATH_DATA_ROOT = r\"~/DATA/petfinder_adoption\"\n",
    "\n",
    "SAMPLE_FRACTION = 1\n",
    "# PATH_OUT = r\"/home/batman/git/hack_sfpd1/Out\"\n",
    "# PATH_OUT_KDE = r\"/home/batman/git/hack_sfpd1/out_kde\"\n",
    "# PATH_REPORTING = r\"/home/batman/git/hack_sfpd1/Reporting\"\n",
    "# PATH_MODELS = r\"/home/batman/git/hack_sfpd4/models\"\n",
    "# TITLE_FONT = {'fontname': 'helvetica'}\n",
    "\n",
    "\n",
    "# TITLE_FONT_NAME = \"Arial\"\n",
    "# plt.rc('font', family='Helvetica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "# =============================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# ML imports\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.model_selection\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Metric\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "def kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Custom imports\n",
    "# =============================================================================\n",
    "import kaggle_utils.transformers as trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Logging\n",
    "# =============================================================================\n",
    "import logging\n",
    "#Delete Jupyter notebook root logger handler\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "\n",
    "# Set level\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter\n",
    "#FORMAT = \"%(asctime)s - %(levelno)-3s - %(module)-10s  %(funcName)-10s: %(message)s\"\n",
    "#FORMAT = \"%(asctime)s - %(levelno)-3s - %(funcName)-10s: %(message)s\"\n",
    "#FORMAT = \"%(asctime)s - %(funcName)-10s: %(message)s\"\n",
    "FORMAT = \"%(asctime)s : %(message)s\"\n",
    "DATE_FMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "#DATE_FMT = \"%H:%M:%S\"\n",
    "formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "\n",
    "# Create handler and assign\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]\n",
    "logging.debug(\"Logging started\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Data source and paths\n",
    "# =============================================================================\n",
    "path_data = Path(PATH_DATA_ROOT, r\"\").expanduser()\n",
    "assert path_data.exists()\n",
    "logging.info(\"Data path {}\".format(PATH_DATA_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# =============================================================================\n",
    "logging.info(f\"Loading files into memory\")\n",
    "\n",
    "# def load_zip\n",
    "# with zipfile.ZipFile(path_data / \"train.zip\").open(\"train.csv\") as f:\n",
    "#     df_train = pd.read_csv(f, delimiter=',')\n",
    "# with zipfile.ZipFile(path_data / \"test.zip\").open(\"test.csv\") as f:\n",
    "#     df_test = pd.read_csv(f, delimiter=',')\n",
    "\n",
    "df_train = pd.read_csv(path_data / 'train.csv')\n",
    "df_test = pd.read_csv(path_data / 'test' / 'test.cs')\n",
    "\n",
    "breeds = pd.read_csv(path_data / \"breed_labels.csv\")\n",
    "colors = pd.read_csv(path_data / \"color_labels.csv\")\n",
    "states = pd.read_csv(path_data / \"state_labels.csv\")\n",
    "\n",
    "logging.debug(\"Loaded train {}\".format(df_train.shape))\n",
    "logging.debug(\"Loaded test {}\".format(df_test.shape))\n",
    "\n",
    "# Add a column to label the source of the data\n",
    "df_train['dataset_type'] = 'train'\n",
    "df_test['dataset_type'] = 'test'\n",
    "\n",
    "logging.debug(\"Added dataset_type column for origin\".format())\n",
    "df_all = pd.concat([df_train, df_test], sort=False)\n",
    "df_all.set_index('PetID',inplace=True)\n",
    "\n",
    "del df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Memory of the training DF:"
   },
   "outputs": [],
   "source": [
    "logging.debug(\"Size of df_all: {} MB\".format(sys.getsizeof(df_all) / 1000 / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['PhotoAmt'] = df_all['PhotoAmt'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Category Mappings"
   },
   "outputs": [],
   "source": [
    "label_maps = dict()\n",
    "label_maps['Vaccinated'] = {\n",
    "    1 : 'Yes',\n",
    "    2 : 'No',\n",
    "    3 : 'Not sure',\n",
    "}\n",
    "label_maps['Type'] = {\n",
    "    1:\"Dog\",\n",
    "    2:\"Cat\"\n",
    "}\n",
    "label_maps['AdoptionSpeed'] = {\n",
    "    0 : \"same day\",\n",
    "    1 : \"between 1 and 7 days\",\n",
    "    2 : \"between 8 and 30 days\",\n",
    "    3 : \"between 31 and 90 days\",\n",
    "    4 : \"No adoption after 100 days\",\n",
    "}\n",
    "label_maps['Gender'] = {\n",
    "    1 : 'Male',\n",
    "    2 : 'Female',\n",
    "    3 : 'Group',\n",
    "}\n",
    "label_maps['MaturitySize'] = {\n",
    "    1 : 'Small',\n",
    "    2 : 'Medium',\n",
    "    3 : 'Large',\n",
    "    4 : 'Extra Large',\n",
    "    0 : 'Not Specified',\n",
    "}\n",
    "label_maps['FurLength'] = {\n",
    "    1 : 'Short',\n",
    "    2 : 'Medium',\n",
    "    3 : 'Long',\n",
    "    0 : 'Not Specified',\n",
    "}\n",
    "label_maps['Dewormed'] = {\n",
    "    1 : 'Yes',\n",
    "    2 : 'No',\n",
    "    3 : 'Not sure',\n",
    "}\n",
    "label_maps['Sterilized'] = {\n",
    "    1 : 'Yes',\n",
    "    2 : 'No',\n",
    "    3 : 'Not sure',\n",
    "}\n",
    "label_maps['Health'] = {\n",
    "    1 : 'Healthy',\n",
    "    2 : 'Minor Injury',\n",
    "    3 : 'Serious Injury',\n",
    "    0 : 'Not Specified',\n",
    "}\n",
    "\n",
    "# For the breeds, load the two types seperate\n",
    "dog_breed = breeds[['BreedID','BreedName']][breeds['Type']==1].copy()\n",
    "map_dog_breed = dict(zip(dog_breed['BreedID'], dog_breed['BreedName']))\n",
    "\n",
    "cat_breed = breeds[['BreedID','BreedName']][breeds['Type']==2].copy()\n",
    "map_cat_breed = dict(zip(cat_breed['BreedID'], cat_breed['BreedName']))\n",
    "\n",
    "# Just in case, check for overlap in breeds\n",
    "# for i in range(308):\n",
    "#     print(i,end=\": \")\n",
    "#     if i in map_dog_breed: print(map_dog_breed[i], end=' - ')\n",
    "#     if i in map_cat_breed: print(map_cat_breed[i], end=' - ')\n",
    "#     if i in map_dog_breed and i in map_cat_breed: raise\n",
    "#     print()\n",
    "\n",
    "# It's fine, join them into one dict\n",
    "map_all_breeds = dict()\n",
    "map_all_breeds.update(map_dog_breed)\n",
    "map_all_breeds.update(map_cat_breed)\n",
    "map_all_breeds[0] = \"NA\"\n",
    "\n",
    "# Now add them to the master label dictionary for each column\n",
    "label_maps['Breed1'] = map_all_breeds\n",
    "label_maps['Breed2'] = map_all_breeds\n",
    "\n",
    "# Similarly, load the color map\n",
    "map_colors = dict(zip(colors['ColorID'], colors['ColorName']))\n",
    "map_colors[0] = \"NA\"\n",
    "label_maps['Color1'] = map_colors\n",
    "label_maps['Color2'] = map_colors\n",
    "label_maps['Color3'] = map_colors\n",
    "\n",
    "# And the states map\n",
    "label_maps['State'] = dict(zip(states['StateID'], states['StateName']))\n",
    "\n",
    "logging.debug(\"Category mappings for {} columns created\".format(len(label_maps)))\n",
    "\n",
    "for map in label_maps:\n",
    "    print(map, label_maps[map])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Dynamically create the transformation definitions"
   },
   "outputs": [],
   "source": [
    "tx_definitions = [(col_name, trf.NumericalToCat(label_maps)) for col_name in label_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Pipeline"
   },
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "# NOTES:\n",
    "# input_df - Ensure the passed in column enters as a series or DF\n",
    "# df_out - Ensure the pipeline returns a df\n",
    "# default - if a column is not transformed, keep it unchanged!\n",
    "# WARNINGS:\n",
    "# The categorical dtype is LOST!\n",
    "# Do NOT use DataFrameMapper for creating new columns, use a regular pipeline!\n",
    "data_mapper = DataFrameMapper(\n",
    "    tx_definitions,\n",
    "input_df=True, df_out=True, default=None)\n",
    "\n",
    "print(\"DataFrameMapper, applies transforms directly selected columns\")\n",
    "for i, step in enumerate(data_mapper.features):\n",
    "    print(i, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0.0,
    "title": "FIT TRANSFORM"
   },
   "outputs": [],
   "source": [
    "df_all = data_mapper.fit_transform(df_all)\n",
    "\n",
    "logging.debug(\"Size of train df_all with string columns: {} MB\".format(sys.getsizeof(df_all)/1000/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2.0,
    "title": "WARNING - sklearn-pandas has a flaw, it does not preserve categorical features!!!"
   },
   "outputs": [],
   "source": [
    "for col in label_maps:\n",
    "    print(col)\n",
    "    df_all[col] = df_all[col].astype('category')\n",
    "logging.debug(\"Reapplied categorical features\".format())\n",
    "logging.debug(\"Size of df_all with categorical features: {} MB\".format(sys.getsizeof(df_all)/1000/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0.0,
    "title": "SUMMARY"
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.debug(\"Final df_all {}\".format(df_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0.0,
    "title": "DONE HERE - DELETE UNUSED"
   },
   "outputs": [],
   "source": [
    "print(\"******************************\")\n",
    "\n",
    "del_vars =[\n",
    "    'breeds',\n",
    "    'cat_breed',\n",
    "    'colors',\n",
    "    'data_mapper',\n",
    "    'dog_breed',\n",
    "    'map_colors',\n",
    "    'map_all_breeds',\n",
    "    'map_cat_breed',\n",
    "    'map_dog_breed',\n",
    "    'states',\n",
    "]\n",
    "cnt = 0\n",
    "for name in dir():\n",
    "    if name in del_vars:\n",
    "        cnt+=1\n",
    "        del globals()[name]\n",
    "logging.info(f\"Removed {cnt} variables from memory\")\n",
    "del cnt, name, del_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Feature\n",
    "# =============================================================================\n",
    "def pure_breed(row):\n",
    "    # print(row)\n",
    "    mixed_breed_keywords = ['domestic', 'tabby', 'mixed']\n",
    "\n",
    "    # Mixed if labelled as such\n",
    "    if row['Breed1'] == 'Mixed Breed':\n",
    "        return False\n",
    "\n",
    "    # Possible pure if no second breed\n",
    "    elif row['Breed2'] == 'NA':\n",
    "        # Reject domestic keywords\n",
    "        if any([word in row['Breed1'].lower() for word in mixed_breed_keywords]):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Build the pipeline"
   },
   "outputs": [],
   "source": [
    "this_pipeline = sk.pipeline.Pipeline([\n",
    "        ('counr', trf.MultipleToNewFeature(['Breed1','Breed2'], 'Pure Breed', pure_breed)),\n",
    "        ])\n",
    "\n",
    "logging.info(\"Created pipeline:\")\n",
    "for i, step in enumerate(this_pipeline.steps):\n",
    "    print(i, step[0], step[1].__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2.0,
    "title": "Fit Transform"
   },
   "outputs": [],
   "source": [
    "original_cols = df_all.columns\n",
    "df_all = this_pipeline.fit_transform(df_all)\n",
    "logging.debug(\"Pipeline complete. {} new columns.\".format(len(df_all.columns) - len(original_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # sample = df_all.iloc[0:10][['Breed1','Breed2']]\n",
    "# df_all['Pure Breed'] = df_all.apply(pure_breed,axis=1)\n",
    "# df_all['Pure Breed'] = df_all['Pure Breed'].astype('category')\n",
    "# df_all.columns\n",
    "# df_all.info()\n",
    "# # For inspection:\n",
    "# # df_breeds = df_all[['Breed1','Breed2','Pure Breed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0.0
   },
   "outputs": [],
   "source": [
    "# r = df_all.sample(10)[['Type']]\n",
    "# len(r)\n",
    "# r[:] = 1\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# this_pipeline = sk.pipeline.Pipeline([\n",
    "#         ('counr', WordCounter('Breed2', 'newcol')),\n",
    "#         ])\n",
    "#\n",
    "# # data_mapper2 = DataFrameMapper(\n",
    "# #     (['Breed1', 'Breed2'], NumericalToCat(None)),\n",
    "# #     input_df=True, df_out=True, default=None)\n",
    "#\n",
    "# logging.info(\"Created pipeline:\")\n",
    "# for i, step in enumerate(this_pipeline.steps):\n",
    "#     print(i, step[0], step[1].__str__()[:60])\n",
    "#\n",
    "# #%%\n",
    "# # transformer_def_list = [\n",
    "# #     (['Breed1', 'Breed2'], MultipleToNewFeature('Test', pure_breed)),\n",
    "# #     # (['Breed1', 'Breed2'], PureBreed()),\n",
    "# #\n",
    "# # ]\n",
    "# #\n",
    "# # transformer_def_list = [\n",
    "# #     (['Breed2'], WordCounter('Breed2', 'newcol')),\n",
    "# #     # (['Breed1', 'Breed2'], PureBreed()),\n",
    "# #\n",
    "# # ]\n",
    "# #\n",
    "# # data_mapper2 = DataFrameMapper(transformer_def_list, input_df=True, df_out=True, default=None)\n",
    "# df_s = df_all.sample(10)[['Breed1', 'Breed2', 'Type']]\n",
    "#\n",
    "# this_pipeline = sk.pipeline.Pipeline([\n",
    "#         ('counr', WordCounter('Breed2', 'newcol')),\n",
    "#         ])\n",
    "#\n",
    "# # data_mapper2 = DataFrameMapper(\n",
    "# #     (['Breed1', 'Breed2'], NumericalToCat(None)),\n",
    "# #     input_df=True, df_out=True, default=None)\n",
    "#\n",
    "# logging.info(\"Created pipeline:\")\n",
    "# for i, step in enumerate(this_pipeline.steps):\n",
    "#     print(i, step[0], step[1].__str__()[:60])\n",
    "#\n",
    "# #%% FIT TRANSFORM\n",
    "# df_s2 = this_pipeline.fit_transform(df_s)\n",
    "#\n",
    "# #%%\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final selection of columns from the main DF\n",
    "cols_to_use = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', 'MaturitySize', 'FurLength',\n",
    "               'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'RescuerID', 'VideoAmt',\n",
    "               'PhotoAmt', 'AdoptionSpeed', 'No_name', 'Pure_breed', 'health', 'Free',\n",
    "               'score', 'magnitude']\n",
    "\n",
    "cols_to_discard = [\n",
    "    'RescuerID',\n",
    "    'Description',\n",
    "    'Name',\n",
    "]\n",
    "\n",
    "\n",
    "logging.debug(\"Feature selection\".format())\n",
    "original_columns = df_all.columns\n",
    "# col_selection = [col for col in all_columns if col not in cols_to_discard]\n",
    "\n",
    "df_all.drop(cols_to_discard,inplace=True, axis=1)\n",
    "\n",
    "logging.debug(\"Selected {} of {} columns\".format(len(df_all.columns),(original_columns)))\n",
    "logging.debug(\"Size of df_all with selected features: {} MB\".format(sys.getsizeof(df_all)/1000/1000))\n",
    "\n",
    "logging.debug(\"Record selection (sampling)\".format())\n",
    "logging.debug(\"{} fraction sampling\".format(SAMPLE_FRACTION))\n",
    "df_all = df_all.sample(frac=SAMPLE_FRACTION)\n",
    "logging.debug(\"Final size of data frame: {}\".format(df_all.shape))\n",
    "logging.debug(\"Size of df_all with selected features and records: {} MB\".format(sys.getsizeof(df_all)/1000/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df_all[df_all['dataset_type']=='train'].copy()\n",
    "df_tr.drop('dataset_type', axis=1, inplace=True)\n",
    "df_te = df_all[df_all['dataset_type']=='test'].copy()\n",
    "df_te.drop('dataset_type', axis=1, inplace=True)\n",
    "\n",
    "y_tr = df_tr['AdoptionSpeed']\n",
    "logging.debug(\"y_tr {}\".format(y_tr.shape))\n",
    "\n",
    "X_tr = df_tr.drop(['AdoptionSpeed'], axis=1)\n",
    "logging.debug(\"X_tr {}\".format(X_tr.shape))\n",
    "\n",
    "X_te = df_te.drop(['AdoptionSpeed'], axis=1)\n",
    "logging.debug(\"X_te {}\".format(X_te.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0.0,
    "title": "DONE HERE - DELETE UNUSED"
   },
   "outputs": [],
   "source": [
    "print(\"******************************\")\n",
    "\n",
    "del_vars =[\n",
    "    'df_all',\n",
    "    'df_tr',\n",
    "    'df_te',\n",
    "]\n",
    "cnt = 0\n",
    "for name in dir():\n",
    "    if name in del_vars:\n",
    "        cnt+=1\n",
    "        del globals()[name]\n",
    "logging.info(f\"Removed {cnt} variables from memory\")\n",
    "del cnt, name, del_vars\n",
    "\n",
    "# Train 2 seperate models, one for cats, one for dogs!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = sk.model_selection.StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(X, X_test, y, params, folds, model_type, plot_feature_importance=False,\n",
    "                averaging='usual', make_oof=False):\n",
    "\n",
    "    logging.debug(\"Starting training {}\".format(model_type))\n",
    "    result_dict = {}\n",
    "    if make_oof:\n",
    "        oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        gc.collect()\n",
    "        print('Fold', fold_n + 1, 'started at', time.ctime())\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        if model_type == 'lgb':\n",
    "            # logging.debug(\"Categorical column selection here!! TODO: NB\".format())\n",
    "            # cat_cols = X.columns.to_list\n",
    "            # X_tr.columns.to_list()\n",
    "\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "            model = lgb.train(params,\n",
    "                              train_data,\n",
    "                              num_boost_round=2000,\n",
    "                              valid_sets=[train_data, valid_data],\n",
    "                              verbose_eval=100,\n",
    "                              early_stopping_rounds=200)\n",
    "\n",
    "            del train_data, valid_data\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration).argmax(1)\n",
    "            del X_valid\n",
    "            gc.collect()\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration).argmax(1)\n",
    "\n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200,\n",
    "                              verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "        if model_type == 'lcv':\n",
    "            model = LogisticRegressionCV(scoring='neg_log_loss', cv=3, multi_class='multinomial')\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostClassifier(iterations=20000, loss_function='MultiClass', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True,\n",
    "                      verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test).reshape(-1, )\n",
    "\n",
    "        if make_oof:\n",
    "            oof[valid_index] = y_pred_valid.reshape(-1, )\n",
    "\n",
    "        scores.append(kappa(y_valid, y_pred_valid))\n",
    "        print('Fold kappa:', kappa(y_valid, y_pred_valid))\n",
    "        print('')\n",
    "\n",
    "        if averaging == 'usual':\n",
    "            prediction += y_pred\n",
    "        elif averaging == 'rank':\n",
    "            prediction += pd.Series(y_pred).rank().values\n",
    "\n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importance()\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "\n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "\n",
    "    if model_type == 'lgb':\n",
    "\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= n_fold\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            # plt.figure(figsize=(16, 12));\n",
    "            # sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            # plt.title('LGB Features (avg over folds)');\n",
    "\n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "\n",
    "    result_dict['prediction'] = prediction\n",
    "    if make_oof:\n",
    "        result_dict['oof'] = oof\n",
    "\n",
    "    return result_dict\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 128,\n",
    "        #  'min_data_in_leaf': 60,\n",
    "         'objective': 'multiclass',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.05,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 3,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "        #  \"lambda_l1\": 0.1,\n",
    "         # \"lambda_l2\": 0.1,\n",
    "         \"random_state\": 42,\n",
    "         \"verbosity\": -1,\n",
    "         \"num_class\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [],
   "source": [
    "\n",
    "# X_tr.info()\n",
    "# y_tr.astype('int')\n",
    "# y_tr.dtype\n",
    "# y_factors = y_tr.factorize()[0]\n",
    "y_integers = y_tr.cat.codes\n",
    "result_dict_lgb = train_model(X=X_tr,\n",
    "                              X_test=X_te,\n",
    "                              y=y_integers,\n",
    "                              params=params,\n",
    "                              folds=folds,\n",
    "                              model_type='lgb',\n",
    "                              plot_feature_importance=True,\n",
    "                              make_oof=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2.0,
    "title": "RESULTS"
   },
   "outputs": [],
   "source": [
    "# r = result_dict_lgb['feature_importance']\n",
    "\n",
    "# cols = result_dict_lgb['feature_importance'][[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "#                 by=\"importance\", ascending=False)[:50].index\n",
    "#\n",
    "# best_features = result_dict_lgb['feature_importance'].loc[result_dict_lgb['feature_importance'].feature.isin(cols)]\n",
    "#\n",
    "# p = plt.figure(figsize=(16, 12))\n",
    "# sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "# plt.title('LGB Features (avg over folds)')\n",
    "# plt.show()#%% Open the submission\n",
    "# with zipfile.ZipFile(path_data / \"test.zip\").open(\"sample_submission.csv\") as f:\n",
    "#     df_submission = pd.read_csv(f, delimiter=',')\n",
    "df_submission = pd.read_csv(path_data / 'test' / 'sample_submission.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Collect predicitons"
   },
   "outputs": [],
   "source": [
    "prediction = (result_dict_lgb['prediction'])\n",
    "submission = pd.DataFrame({'PetID': df_submission.PetID, 'AdoptionSpeed': [int(i) for i in prediction]})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Create csv"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
