{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:07.212275Z",
     "start_time": "2019-03-28T09:14:05.541077Z"
    },
    "_uuid": "a194102854a9eaec85445c81d0621d55f1476560",
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "from sklearn.model_selection import StratifiedKFold,KFold,GroupKFold\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "from gensim.sklearn_api.ldamodel import LdaTransformer\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim import corpora\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:07.639786Z",
     "start_time": "2019-03-28T09:14:07.219878Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "isascii = lambda s: len(s) == len(s.encode())\n",
    "tknzr = TweetTokenizer()\n",
    "import jieba\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:08.040365Z",
     "start_time": "2019-03-28T09:14:07.641269Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch imports\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torchvision.models import resnet50, resnet34, densenet201, densenet121\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:08.075019Z",
     "start_time": "2019-03-28T09:14:08.043517Z"
    },
    "code_folding": [
     0,
     1,
     17,
     31,
     57,
     63,
     68,
     71,
     84,
     98
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# util funcs\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    min_rating, max_rating =None, None\n",
    "    rater_a, rater_b = np.array(y, dtype=int), np.array(y_pred, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b, min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator, denominator = 0.0, 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j] / num_scored_items)\n",
    "            d = np.square(i - j) / np.square(num_ratings - 1)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)\n",
    "\n",
    "def val_kappa(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    preds = np.argmax(preds.reshape((-1,5)), axis=1)\n",
    "    \n",
    "    return 'qwk', quadratic_weighted_kappa(labels, preds), True\n",
    "\n",
    "def val_kappa_reg(preds, train_data, cdf):\n",
    "    labels = train_data.get_label()\n",
    "    preds = getTestScore2(preds, cdf)\n",
    "    return 'qwk', quadratic_weighted_kappa(labels, preds), True\n",
    "\n",
    "def get_cdf(hist):\n",
    "    return np.cumsum(hist/np.sum(hist))\n",
    "\n",
    "def getScore(pred, cdf, valid=False):\n",
    "    num = pred.shape[0]\n",
    "    output = np.asarray([4]*num, dtype=int)\n",
    "    rank = pred.argsort()\n",
    "    output[rank[:int(num*cdf[0]-1)]] = 0\n",
    "    output[rank[int(num*cdf[0]):int(num*cdf[1]-1)]] = 1\n",
    "    output[rank[int(num*cdf[1]):int(num*cdf[2]-1)]] = 2\n",
    "    output[rank[int(num*cdf[2]):int(num*cdf[3]-1)]] = 3\n",
    "    if valid:\n",
    "        cutoff = [ pred[rank[int(num*cdf[i]-1)]] for i in range(4) ]\n",
    "        return output, cutoff\n",
    "    return output\n",
    "\n",
    "def getTestScore(pred, cutoff):\n",
    "    num = pred.shape[0]\n",
    "    output = np.asarray([4]*num, dtype=int)\n",
    "    for i in range(num):\n",
    "        if pred[i] <= cutoff[0]:\n",
    "            output[i] = 0\n",
    "        elif pred[i] <= cutoff[1]:\n",
    "            output[i] = 1\n",
    "        elif pred[i] <= cutoff[2]:\n",
    "            output[i] = 2\n",
    "        elif pred[i] <= cutoff[3]:\n",
    "            output[i] = 3\n",
    "    return output\n",
    "\n",
    "def getTestScore2(pred, cdf):\n",
    "    num = pred.shape[0]\n",
    "    rank = pred.argsort()\n",
    "    output = np.asarray([4]*num, dtype=int)\n",
    "    output[rank[:int(num*cdf[0]-1)]] = 0\n",
    "    output[rank[int(num*cdf[0]):int(num*cdf[1]-1)]] = 1\n",
    "    output[rank[int(num*cdf[1]):int(num*cdf[2]-1)]] = 2\n",
    "    output[rank[int(num*cdf[2]):int(num*cdf[3]-1)]] = 3\n",
    "    return output\n",
    "\n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "isascii = lambda s: len(s) == len(s.encode())\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    init_doc = tknzr.tokenize(text)\n",
    "    retval = []\n",
    "    for t in init_doc:\n",
    "        if isascii(t): \n",
    "            retval.append(t)\n",
    "        else:\n",
    "            for w in t:\n",
    "                retval.append(w)\n",
    "    return retval\n",
    "\n",
    "def build_emb_matrix(word_dict, emb_dict):\n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict)+1000\n",
    "    nb_oov = 0\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = emb_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = emb_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = emb_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = emb_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = emb_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = emb_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = emb_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        nb_oov+=1\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector                    \n",
    "    return embedding_matrix, nb_words, nb_oov\n",
    "\n",
    "def _init_esim_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_normal_(module.weight.data)\n",
    "        nn.init.constant_(module.bias.data, 0.0)\n",
    "\n",
    "    elif isinstance(module, nn.LSTM) or isinstance(module, nn.GRU):\n",
    "        if isinstance(module, nn.LSTM):\n",
    "            hidden_size = module.bias_hh_l0.data.shape[0] // 4\n",
    "        else:\n",
    "            hidden_size = module.bias_hh_l0.data.shape[0] // 3\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif '_ih_' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif '_hh_' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                param.data[hidden_size:(2 * hidden_size)] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:08.083745Z",
     "start_time": "2019-03-28T09:14:08.077043Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "    \n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n",
    "        return -cohen_kappa_score(y, preds, weights='quadratic')\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X = X, y = y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "    \n",
    "    def predict(self, X, coef):\n",
    "        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n",
    "        return preds\n",
    "    \n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:14.144709Z",
     "start_time": "2019-03-28T09:14:14.122549Z"
    },
    "code_folding": [
     0,
     33
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ResnetModel(nn.Module):\n",
    "    def __init__(self, resnet_fun = resnet50, freeze_basenet = True):\n",
    "        super(ResnetModel, self).__init__()\n",
    "        self.resnet = resnet_fun(pretrained=False)\n",
    "        if freeze_basenet:\n",
    "            for p in self.resnet.parameters():\n",
    "                p.requires_grad = False\n",
    "       \n",
    "    def init_resnet(self, path):\n",
    "        state = torch.load(path)\n",
    "        self.resnet.load_state_dict(state)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x/255.0\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        x = torch.cat([\n",
    "            (x[:, [0]] - mean[0]) / std[0],\n",
    "            (x[:, [1]] - mean[1]) / std[1],\n",
    "            (x[:, [2]] - mean[2]) / std[2],\n",
    "        ], 1)\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        x = F.adaptive_avg_pool2d(x, output_size=1).view(batch_size, -1)\n",
    "        return x\n",
    "    \n",
    "class DenseModel(nn.Module):\n",
    "    def __init__(self, dense_func = densenet201, freeze_basenet = True):\n",
    "        super(DenseModel, self).__init__()\n",
    "        self.densenet = dense_func(pretrained=False)\n",
    "#         model_name = 'se_resnet50'\n",
    "#         self.resnet = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained=None)\n",
    "        if freeze_basenet:\n",
    "            for p in self.densenet.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "    def init_densenet(self, path):\n",
    "        state = torch.load(path)\n",
    "        self.densenet.load_state_dict(state)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x/255.0\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        x = torch.cat([\n",
    "            (x[:, [0]] - mean[0]) / std[0],\n",
    "            (x[:, [1]] - mean[1]) / std[1],\n",
    "            (x[:, [2]] - mean[2]) / std[2],\n",
    "        ], 1)\n",
    "        x = self.densenet.features(x)\n",
    "        x = F.adaptive_avg_pool2d(F.relu(x,inplace=True), output_size=1).view(batch_size, -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:15.186506Z",
     "start_time": "2019-03-28T09:14:15.174203Z"
    },
    "code_folding": [
     5,
     16
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def img_to_torch(image):\n",
    "    return torch.from_numpy(np.transpose(image, (2, 0, 1)))\n",
    "\n",
    "def pad_to_square(image):\n",
    "    h, w = image.shape[0:2]\n",
    "    new_size = max(h, w)\n",
    "    delta_top = (new_size-h)//2\n",
    "    delta_bottom = new_size-h-delta_top\n",
    "    delta_left = (new_size-w)//2\n",
    "    delta_right = new_size-delta_left-w\n",
    "    new_im = cv2.copyMakeBorder(image, delta_top, delta_bottom, delta_left, delta_right, \n",
    "                                cv2.BORDER_CONSTANT,  value=[0,0,0])\n",
    "    return new_im\n",
    "\n",
    "class PetDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.path = df['path'].tolist()\n",
    "        self.cache = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index not in range(0, len(self.df)):\n",
    "            return self.__getitem__(np.random.randint(0, self.__len__()))\n",
    "        \n",
    "        # only take on channel\n",
    "#         if index not in self.cache:\n",
    "        image = cv2.imread(self.path[index])\n",
    "        image = pad_to_square(image)\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "#             self.cache[index] = img_to_torch(image)\n",
    "\n",
    "        return img_to_torch(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f16a86529d54b5fbc3d24569da81077abab9ba21",
    "heading_collapsed": true
   },
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:17.851820Z",
     "start_time": "2019-03-28T09:14:17.841750Z"
    },
    "_uuid": "8473644b13e590420cdc349f7fb27a541fb6644b",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.listdir('../input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:19.626978Z",
     "start_time": "2019-03-28T09:14:19.611641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels_state = pd.read_csv('../input/petfinder-adoption-prediction/color_labels.csv')\n",
    "labels_color = pd.read_csv('../input/petfinder-adoption-prediction/state_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:20.301977Z",
     "start_time": "2019-03-28T09:14:19.940640Z"
    },
    "_uuid": "54985b02f752cd86dd2a39ac261281573ca9e81d",
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "train = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\n",
    "test = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n",
    "sample_submission = pd.read_csv('../input/petfinder-adoption-prediction/test/sample_submission.csv')\n",
    "train_len = len(train)\n",
    "data_df = pd.concat([train, test], sort=False).reset_index(drop=True)\n",
    "data_df['Breed_full'] = data_df['Breed1'].astype(str)+'_'+data_df['Breed2'].astype(str)\n",
    "data_df['Color_full'] = data_df['Color1'].astype(str)+'_'+data_df['Color2'].astype(str)+'_'+data_df['Color3'].astype(str)\n",
    "\n",
    "data_df['Breed_full'],_ = pd.factorize(data_df['Breed_full'])\n",
    "data_df['Color_full'],_ = pd.factorize(data_df['Color_full'])\n",
    "data_df['State'],_ = pd.factorize(data_df['State'])\n",
    "\n",
    "data_df['hard_interaction'] = data_df['Type'].astype(str)+data_df['Gender'].astype(str)+ \\\n",
    "                              data_df['Vaccinated'].astype(str)+'_'+ \\\n",
    "                              data_df['Dewormed'].astype(str)+'_'+data_df['Sterilized'].astype(str)\n",
    "data_df['hard_interaction'],_ = pd.factorize(data_df['hard_interaction'])\n",
    "\n",
    "data_df['MaturitySize'] = data_df['MaturitySize'].replace(0, np.nan)\n",
    "data_df['FurLength'] = data_df['FurLength'].replace(0, np.nan)\n",
    "\n",
    "data_df['Vaccinated'] = data_df['Vaccinated'].replace(3, np.nan)\n",
    "data_df['Vaccinated'] = data_df['Vaccinated'].replace(2, 0)\n",
    "\n",
    "data_df['Dewormed'] = data_df['Dewormed'].replace(3, np.nan)\n",
    "data_df['Dewormed'] = data_df['Dewormed'].replace(2, 0)\n",
    "\n",
    "data_df['Sterilized'] = data_df['Sterilized'].replace(3, np.nan)\n",
    "data_df['Sterilized'] = data_df['Sterilized'].replace(2, 0)\n",
    "\n",
    "\n",
    "data_df['Health'] = data_df['Health'].replace(0, np.nan)\n",
    "data_df['age_in_year'] = (data_df['Age']//12).astype(np.int8)\n",
    "data_df['avg_fee'] = data_df['Fee']/data_df['Quantity']\n",
    "data_df['avg_photo'] = data_df['PhotoAmt']/data_df['Quantity']\n",
    "\n",
    "# name feature\n",
    "pattern = re.compile(r\"[0-9\\.:!]\")\n",
    "data_df['empty_name'] = data_df['Name'].isnull().astype(np.int8)\n",
    "data_df['Name'] =data_df['Name'].fillna('')\n",
    "data_df['name_len'] = data_df['Name'].apply(lambda x: len(x))\n",
    "data_df['strange_name'] = data_df['Name'].apply(lambda x: len(pattern.findall(x))>0).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:20.347851Z",
     "start_time": "2019-03-28T09:14:20.321185Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_df['color_num'] = 1\n",
    "data_df['color_num'] += data_df['Color2'].apply(lambda x: 1 if x!=0 else 0)\n",
    "data_df['color_num'] += data_df['Color3'].apply(lambda x: 1 if x!=0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:20.627366Z",
     "start_time": "2019-03-28T09:14:20.614781Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# breed feature\n",
    "labels_breed = pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv')\n",
    "labels_breed.rename(index=str, columns={'BreedName':'Breed1Name'},inplace=True)\n",
    "labels_breed['Breed2Name'] = labels_breed['Breed1Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:20.958522Z",
     "start_time": "2019-03-28T09:14:20.874337Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_df = data_df.merge(labels_breed[['BreedID','Breed1Name']], left_on='Breed1', right_on='BreedID', how='left')\n",
    "data_df.drop('BreedID',axis=1,inplace=True)\n",
    "data_df = data_df.merge(labels_breed[['BreedID','Breed2Name']], left_on='Breed2', right_on='BreedID', how='left')\n",
    "data_df.drop('BreedID',axis=1,inplace=True)\n",
    "data_df['Breed2Name'].fillna('',inplace=True)\n",
    "data_df['BreedName_full'] = data_df['Breed1Name']+' '+data_df['Breed2Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:21.193322Z",
     "start_time": "2019-03-28T09:14:21.179520Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_df['breed_noname'] = data_df['BreedName_full'].isnull().astype(np.int8)\n",
    "data_df['BreedName_full'].fillna('',inplace=True)\n",
    "data_df['BreedName_full'] = data_df['BreedName_full'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:22.309745Z",
     "start_time": "2019-03-28T09:14:22.275175Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_df['breed_num'] = 1\n",
    "data_df['breed_num'] += data_df['Breed2'].apply(lambda x: 1 if x!=0 else 0)\n",
    "data_df['breed_mixed'] = data_df['BreedName_full'].apply(lambda x: x.find('mixed')>=0).astype(np.int8)\n",
    "data_df['breed_Domestic'] = data_df['BreedName_full'].apply(lambda x: x.find('domestic')>=0).astype(np.int8)\n",
    "data_df['pure_breed'] = ((data_df['breed_num']==1)&(data_df['breed_mixed']==0)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:22.703842Z",
     "start_time": "2019-03-28T09:14:22.699116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# data_df['breed_American'] = data_df['BreedName_full'].apply(lambda x: x.find('american')>=0).astype(np.int8)\n",
    "# data_df['breed_Australian'] = data_df['BreedName_full'].apply(lambda x: x.find('australian')>=0).astype(np.int8)\n",
    "# data_df['breed_Belgian'] = data_df['BreedName_full'].apply(lambda x: x.find('belgian')>=0).astype(np.int8)\n",
    "# data_df['breed_English'] = data_df['BreedName_full'].apply(lambda x: x.find('english')>=0).astype(np.int8)\n",
    "# data_df['breed_German'] = data_df['BreedName_full'].apply(lambda x: x.find('german')>=0).astype(np.int8)\n",
    "# data_df['breed_irish'] = data_df['BreedName_full'].apply(lambda x: x.find('irish')>=0).astype(np.int8)\n",
    "# \n",
    "# data_df['breed_Oriental'] = data_df['BreedName_full'].apply(lambda x: x.find('Oriental')>=0).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:26.280799Z",
     "start_time": "2019-03-28T09:14:26.272615Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# add features from ratings \n",
    "with open('../input/cat-and-dog-breeds-parameters/rating.json', 'r') as f:\n",
    "    ratings = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:26.679336Z",
     "start_time": "2019-03-28T09:14:26.673362Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cat_ratings = ratings['cat_breeds']\n",
    "dog_ratings = ratings['dog_breeds']\n",
    "catdog_ratings = {**cat_ratings, **dog_ratings} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:28.308924Z",
     "start_time": "2019-03-28T09:14:27.683832Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parameters_df=pd.DataFrame()\n",
    "i = 0\n",
    "for breed in catdog_ratings.keys():\n",
    "    for key in catdog_ratings[breed].keys():\n",
    "        parameters_df.at[i,'breed'] = breed\n",
    "        parameters_df.at[i,key] = catdog_ratings[breed][key] \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:28.319961Z",
     "start_time": "2019-03-28T09:14:28.312509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parameters_df.rename(index=str, columns={'breed':'Breed1Name'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:32.422754Z",
     "start_time": "2019-03-28T09:14:32.384843Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_df = data_df.merge(parameters_df[['Breed1Name','Affectionate with Family','Amount of Shedding',\n",
    "                                      'Easy to Groom','General Health','Intelligence','Kid Friendly',\n",
    "                                      'Pet Friendly','Potential for Playfulness']], on='Breed1Name', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:14:33.731367Z",
     "start_time": "2019-03-28T09:14:33.722663Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_df['Description'] = data_df['Description'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:15:22.405314Z",
     "start_time": "2019-03-28T09:14:34.174297Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "english_desc, chinese_desc = [], []\n",
    "tokens = set()\n",
    "word_dict = {}\n",
    "pos_count, word_count = 1, 1 # starts from 1, 0 for padding token\n",
    "pos_dict = {}\n",
    "eng_sequences = []\n",
    "pos_sequences = []\n",
    "for i in range(len(data_df)):\n",
    "    e_d, c_d, eng_seq, pos_seq = [], [], [], []\n",
    "    doc = custom_tokenizer(data_df['Description'].iloc[i])\n",
    "    for token in doc:\n",
    "        if not isascii(token):\n",
    "            c_d.append(token)\n",
    "        else:\n",
    "            e_d.append(token)\n",
    "            if token not in word_dict:\n",
    "                word_dict[token] = word_count\n",
    "                word_count +=1\n",
    "    english_desc.append(' '.join(e_d))\n",
    "    chinese_desc.append(' '.join(c_d))\n",
    "    pos_seq = nltk.pos_tag(e_d)\n",
    "    for t in pos_seq:\n",
    "        if t[1] not in pos_dict:\n",
    "            pos_dict[t[1]] = pos_count\n",
    "            pos_count += 1\n",
    "    pos_seq = [pos_dict[t[1]] for t in pos_seq]\n",
    "    eng_seq = [word_dict[t] for t in e_d]\n",
    "    if len(eng_seq)==0:\n",
    "        eng_seq.append(0)\n",
    "        pos_seq.append(0)\n",
    "    eng_sequences.append(eng_seq)\n",
    "    pos_sequences.append(pos_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:15:22.702452Z",
     "start_time": "2019-03-28T09:15:22.407947Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_df['English_desc'] = english_desc\n",
    "data_df['Chinese_desc'] = chinese_desc\n",
    "\n",
    "data_df['e_description_len'] = data_df['English_desc'].apply(lambda x:len(x))\n",
    "data_df['e_description_word_len'] = data_df['English_desc'].apply(lambda x: len(x.split(' ')))\n",
    "data_df['e_description_word_unique'] = data_df['English_desc'].apply(lambda x: len(set(x.split(' '))))\n",
    "\n",
    "data_df['c_description_len'] = data_df['Chinese_desc'].apply(lambda x:len(x))\n",
    "data_df['c_description_word_len'] = data_df['Chinese_desc'].apply(lambda x:len(x.split(' ')))\n",
    "data_df['c_description_word_unique'] = data_df['Chinese_desc'].apply(lambda x: len(set(x)))\n",
    "\n",
    "data_df['description_len'] = data_df['Description'].apply(lambda x:len(x))\n",
    "data_df['description_word_len'] = data_df['Description'].apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:15:22.708140Z",
     "start_time": "2019-03-28T09:15:22.704016Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(len(eng_sequences))\n",
    "print(len(pos_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:15:22.724082Z",
     "start_time": "2019-03-28T09:15:22.718290Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_pos = len(pos_dict)\n",
    "print(nb_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:17:22.561196Z",
     "start_time": "2019-03-28T09:15:22.733083Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# build embedding\n",
    "def load_glove():\n",
    "    EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in (open(EMBEDDING_FILE)))\n",
    "    return embeddings_index\n",
    "\n",
    "glove_emb = load_glove()\n",
    "\n",
    "embedding_matrix, nb_words, nb_oov = build_emb_matrix(word_dict, glove_emb)\n",
    "print(nb_words, nb_oov)\n",
    "del glove_emb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T10:51:05.806898Z",
     "start_time": "2019-03-26T10:51:05.773413Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# np.save('mini_embedding.npy',embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T11:14:34.158733Z",
     "start_time": "2019-03-26T11:14:34.138476Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# embedding_matrix = np.load('mini_embedding.npy')\n",
    "# nb_words = 33947"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:17:22.567062Z",
     "start_time": "2019-03-28T09:17:22.563064Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(set(train.index.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:17:22.592743Z",
     "start_time": "2019-03-28T09:17:22.568613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "# kfold = GroupKFold(n_splits=n_splits)\n",
    "split_index = []\n",
    "# for train_idx, valid_idx in kfold.split(train, train['AdoptionSpeed'], train['RescuerID']):\n",
    "#     split_index.append((train_idx, valid_idx))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_splits, random_state=1991)\n",
    "for train_idx, valid_idx in kfold.split(train, train['AdoptionSpeed']):\n",
    "    split_index.append((train_idx, valid_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43025d0dbd7886a7e3d7168a45bec6790307ff44",
    "hidden": true
   },
   "source": [
    "### load mapping dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:17:23.320246Z",
     "start_time": "2019-03-28T09:17:22.594229Z"
    },
    "_uuid": "876557740f8c8ccc2b4c0080acee005fb2eff73f",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_images/*.jpg'))\n",
    "train_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_metadata/*.json'))\n",
    "train_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_sentiment/*.json'))\n",
    "\n",
    "print('num of train images files: {}'.format(len(train_image_files)))\n",
    "print('num of train metadata files: {}'.format(len(train_metadata_files)))\n",
    "print('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n",
    "\n",
    "test_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_images/*.jpg'))\n",
    "test_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_metadata/*.json'))\n",
    "test_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_sentiment/*.json'))\n",
    "\n",
    "print('num of test images files: {}'.format(len(test_image_files)))\n",
    "print('num of test metadata files: {}'.format(len(test_metadata_files)))\n",
    "print('num of test sentiment files: {}'.format(len(test_sentiment_files)))\n",
    "\n",
    "image_files = train_image_files+test_image_files\n",
    "metadata_files = train_metadata_files+test_metadata_files\n",
    "sentiment_files = train_sentiment_files+test_sentiment_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:17:23.670428Z",
     "start_time": "2019-03-28T09:17:23.322142Z"
    }
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def get_petid(path):\n",
    "    basename = os.path.basename(path)\n",
    "    return basename.split('-')[0]\n",
    "def get_picid(path):\n",
    "    basename = os.path.splitext(os.path.basename(path))[0]\n",
    "    return basename.split('-')[1]\n",
    "\n",
    "image_df = pd.DataFrame(image_files, columns=['path'])\n",
    "image_df['PetID'] = image_df['path'].apply(get_petid)\n",
    "image_df['PicID'] = image_df['path'].apply(get_picid)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:17:23.705737Z",
     "start_time": "2019-03-28T09:17:23.673622Z"
    }
   },
   "outputs": [],
   "source": [
    "image_df[image_df['PicID']=='1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:17:23.712302Z",
     "start_time": "2019-03-28T09:17:23.707336Z"
    }
   },
   "outputs": [],
   "source": [
    "pet_image_dataset = PetDataset(image_df)\n",
    "pet_image_loader = DataLoader(pet_image_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                       num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:27:47.988715Z",
     "start_time": "2019-03-28T09:17:23.713916Z"
    }
   },
   "outputs": [],
   "source": [
    "# cat and dog model\n",
    "resnet34_feature = []\n",
    "model = ResnetModel(resnet_fun=resnet34)\n",
    "model.init_resnet('../input/pretrained-pytorch-dog-and-cat-models/resnet34.pth')\n",
    "model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img_batch in tqdm(pet_image_loader):\n",
    "        img_batch = img_batch.float().cuda()\n",
    "        y_pred = model(img_batch)\n",
    "        resnet34_feature.append(y_pred.cpu().numpy()) \n",
    "resnet34_feature = np.vstack(resnet34_feature)\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "densenet121_feature = []\n",
    "model = DenseModel(dense_func=densenet121)\n",
    "model.init_densenet('../input/pretrained-pytorch-dog-and-cat-models/densenet121.pth')\n",
    "model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img_batch in tqdm(pet_image_loader):\n",
    "        img_batch = img_batch.float().cuda()\n",
    "        y_pred = model(img_batch)\n",
    "        densenet121_feature.append(y_pred.cpu().numpy()) \n",
    "densenet121_feature = np.vstack(densenet121_feature)\n",
    "del model\n",
    "# del pet_image_loader\n",
    "# del pet_image_dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:43:56.443636Z",
     "start_time": "2019-03-28T09:27:47.990398Z"
    }
   },
   "outputs": [],
   "source": [
    "densenet_feature = []\n",
    "model = DenseModel()\n",
    "model.init_densenet('../input/pytorch-pretrained-image-models/densenet201.pth')\n",
    "model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img_batch in tqdm(pet_image_loader):\n",
    "        img_batch = img_batch.float().cuda()\n",
    "        y_pred = model(img_batch)\n",
    "        densenet_feature.append(y_pred.cpu().numpy()) \n",
    "densenet_feature = np.vstack(densenet_feature)\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "resnet50_feature = []\n",
    "model = ResnetModel()\n",
    "model.init_resnet('../input/pytorch-pretrained-image-models/resnet50.pth')\n",
    "model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img_batch in tqdm(pet_image_loader):\n",
    "        img_batch = img_batch.float().cuda()\n",
    "        y_pred = model(img_batch)\n",
    "        resnet50_feature.append(y_pred.cpu().numpy()) \n",
    "resnet50_feature = np.vstack(resnet50_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:43:56.443636Z",
     "start_time": "2019-03-28T09:27:47.990398Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del pet_image_loader\n",
    "del pet_image_dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:43:56.450176Z",
     "start_time": "2019-03-28T09:43:56.445426Z"
    }
   },
   "outputs": [],
   "source": [
    "RES50_IMG_FEATURE_DIM = resnet50_feature.shape[1]\n",
    "DENSE_IMG_FEATURE_DIM = densenet_feature.shape[1]\n",
    "RES34_IMG_FEATURE_DIM = resnet34_feature.shape[1]\n",
    "DENSE121_IMG_FEATURE_DIM = densenet121_feature.shape[1]\n",
    "print(RES50_IMG_FEATURE_DIM)\n",
    "print(DENSE_IMG_FEATURE_DIM)\n",
    "print(RES34_IMG_FEATURE_DIM)\n",
    "print(DENSE121_IMG_FEATURE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:44:01.264797Z",
     "start_time": "2019-03-28T09:43:56.452493Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet50_feature_df = pd.DataFrame(resnet50_feature, dtype=np.float32,\n",
    "                                   columns=['resnet50_%d'%i for i in range(RES50_IMG_FEATURE_DIM)])\n",
    "resnet50_feature_df['PetID'] = image_df['PetID'].values\n",
    "resnet50_feature_df['PicID'] = image_df['PicID'].values\n",
    "resnet50_feature_df_avg = resnet50_feature_df.drop('PicID', axis=1).groupby('PetID').agg('mean').reset_index()\n",
    "resnet50_feature_df_avg.columns = ['PetID']+['resnet50_mean_%d'%i for i in range(RES50_IMG_FEATURE_DIM)]\n",
    "resnet50_feature_df_1 = resnet50_feature_df[resnet50_feature_df['PicID']=='1'].drop('PicID', axis=1)\n",
    "resnet50_feature_df = resnet50_feature_df_1.merge(resnet50_feature_df_avg, on='PetID', how='outer')\n",
    "resnet50_feature_df = data_df[['PetID','RescuerID','AdoptionSpeed']].merge(resnet50_feature_df, on='PetID', how='left')\n",
    "for c in resnet50_feature_df.columns:\n",
    "    if c in ['PetID','RescuerID','AdoptionSpeed']:continue\n",
    "    resnet50_feature_df[c] = resnet50_feature_df[c].fillna(-1)\n",
    "del resnet50_feature_df_avg\n",
    "del resnet50_feature_df_1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:44:05.391174Z",
     "start_time": "2019-03-28T09:44:01.266456Z"
    }
   },
   "outputs": [],
   "source": [
    "dense_feature_df = pd.DataFrame(densenet_feature, dtype=np.float32,\n",
    "                                   columns=['densenet_%d'%i for i in range(DENSE_IMG_FEATURE_DIM)])\n",
    "dense_feature_df['PetID'] = image_df['PetID'].values\n",
    "dense_feature_df['PicID'] = image_df['PicID'].values\n",
    "dense_feature_df_avg = dense_feature_df.drop('PicID', axis=1).groupby('PetID').agg('mean').reset_index()\n",
    "dense_feature_df_avg.columns = ['PetID']+['densenet_mean_%d'%i for i in range(DENSE_IMG_FEATURE_DIM)]\n",
    "dense_feature_df_1 = dense_feature_df[dense_feature_df['PicID']=='1'].drop('PicID', axis=1)\n",
    "dense_feature_df = dense_feature_df_1.merge(dense_feature_df_avg, on='PetID', how='left')\n",
    "dense_feature_df = data_df[['PetID','RescuerID','AdoptionSpeed']].merge(dense_feature_df, on='PetID', how='left')\n",
    "for c in dense_feature_df.columns:\n",
    "    if c in ['PetID','RescuerID','AdoptionSpeed']:continue\n",
    "    dense_feature_df[c] = dense_feature_df[c].fillna(-1)\n",
    "del dense_feature_df_1\n",
    "del dense_feature_df_avg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:44:06.516866Z",
     "start_time": "2019-03-28T09:44:05.392571Z"
    }
   },
   "outputs": [],
   "source": [
    "# res34 on pet and dogs\n",
    "res34_feature_df = pd.DataFrame(resnet34_feature, dtype=np.float32,\n",
    "                                   columns=['densenet_%d'%i for i in range(RES34_IMG_FEATURE_DIM)])\n",
    "res34_feature_df['PetID'] = image_df['PetID'].values\n",
    "res34_feature_df['PicID'] = image_df['PicID'].values\n",
    "res34_feature_df_avg = res34_feature_df.drop('PicID', axis=1).groupby('PetID').agg('mean').reset_index()\n",
    "res34_feature_df_avg.columns = ['PetID']+['res34_mean_%d'%i for i in range(RES34_IMG_FEATURE_DIM)]\n",
    "res34_feature_df_1 = res34_feature_df[res34_feature_df['PicID']=='1'].drop('PicID', axis=1)\n",
    "res34_feature_df = res34_feature_df_1.merge(res34_feature_df_avg, on='PetID', how='left')\n",
    "res34_feature_df = data_df[['PetID','RescuerID','AdoptionSpeed']].merge(res34_feature_df, on='PetID', how='left')\n",
    "for c in res34_feature_df.columns:\n",
    "    if c in ['PetID','RescuerID','AdoptionSpeed']:continue\n",
    "    res34_feature_df[c] = res34_feature_df[c].fillna(-1)\n",
    "del res34_feature_df_1\n",
    "del res34_feature_df_avg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:44:08.843986Z",
     "start_time": "2019-03-28T09:44:06.525540Z"
    }
   },
   "outputs": [],
   "source": [
    "# densenet121 on pet and dogs\n",
    "dense121_feature_df = pd.DataFrame(densenet121_feature, dtype=np.float32,\n",
    "                                   columns=['densenet_%d'%i for i in range(DENSE121_IMG_FEATURE_DIM)])\n",
    "dense121_feature_df['PetID'] = image_df['PetID'].values\n",
    "dense121_feature_df['PicID'] = image_df['PicID'].values\n",
    "dense121_feature_df_avg = dense121_feature_df.drop('PicID', axis=1).groupby('PetID').agg('mean').reset_index()\n",
    "dense121_feature_df_avg.columns = ['PetID']+['dense121_mean_%d'%i for i in range(DENSE121_IMG_FEATURE_DIM)]\n",
    "dense121_feature_df_1 = dense121_feature_df[dense121_feature_df['PicID']=='1'].drop('PicID', axis=1)\n",
    "dense121_feature_df = dense121_feature_df_1.merge(dense121_feature_df_avg, on='PetID', how='left')\n",
    "dense121_feature_df = data_df[['PetID','RescuerID','AdoptionSpeed']].merge(dense121_feature_df, on='PetID', how='left')\n",
    "for c in dense121_feature_df.columns:\n",
    "    if c in ['PetID','RescuerID','AdoptionSpeed']:continue\n",
    "    dense121_feature_df[c] = dense121_feature_df[c].fillna(-1)\n",
    "del dense121_feature_df_1\n",
    "del dense121_feature_df_avg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T09:44:08.862349Z",
     "start_time": "2019-03-28T09:44:08.856969Z"
    },
    "_kg_hide-input": true,
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ImgExtractor(nn.Module):\n",
    "    def __init__(self, raw_feature_dim = 4096, feature_dim=64):\n",
    "        super(ImgExtractor, self).__init__()\n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.BatchNorm1d(raw_feature_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(raw_feature_dim, raw_feature_dim//4),\n",
    "            nn.ELU(inplace=True),\n",
    "#             nn.BatchNorm1d(raw_feature_dim//4),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(raw_feature_dim//4, raw_feature_dim//8),\n",
    "            nn.ELU(inplace=True),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.BatchNorm1d(raw_feature_dim//8),\n",
    "            nn.Linear(raw_feature_dim//8, feature_dim),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 1)\n",
    "        )\n",
    "        \n",
    "#         self.apply(_init_esim_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        feat = self.extractor(x)\n",
    "        out = self.logit(feat)\n",
    "        return out, feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:42:29.181216Z",
     "start_time": "2019-03-28T10:42:29.147501Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "IMG_FEATURE_DIM_NN = 128\n",
    "IMG_FEATURE_DIM_NN2 = 64\n",
    "\n",
    "def get_image_feature(img_train_data, img_test_data,img_feature_dim):\n",
    "    loss_fn = torch.nn.MSELoss().cuda()\n",
    "\n",
    "    oof_train_img = np.zeros((img_train_data.shape[0], img_feature_dim+1))\n",
    "    oof_test_img = []\n",
    "\n",
    "    # X_test = raw_img_features_test.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1)\n",
    "    test_dataset = TensorDataset(torch.tensor(img_test_data))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "    qwks = []\n",
    "    rmses = []\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(split_index): \n",
    "        print('fold:',n_fold)\n",
    "        X_tr = img_train_data[train_idx]\n",
    "        X_val = img_train_data[valid_idx]\n",
    "\n",
    "        y_tr = train.iloc[train_idx]['AdoptionSpeed'].values    \n",
    "        y_val = train.iloc[valid_idx]['AdoptionSpeed'].values\n",
    "\n",
    "        hist = histogram(y_tr.astype(int), \n",
    "                         int(np.min(train['AdoptionSpeed'])), \n",
    "                         int(np.max(train['AdoptionSpeed'])))\n",
    "        tr_cdf = get_cdf(hist)\n",
    "\n",
    "        tra_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr))\n",
    "        val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
    "\n",
    "        training_loader = DataLoader(tra_dataset, batch_size=1024, shuffle=True, num_workers=1, pin_memory=True)\n",
    "        validation_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "        model = ImgExtractor(raw_feature_dim=img_train_data.shape[1], feature_dim=img_feature_dim)\n",
    "        model.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.002)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=7, eta_min=0.0003)\n",
    "        iteration = 0\n",
    "        min_val_loss = 100\n",
    "        since = time.time()\n",
    "        test_feats = None\n",
    "        for epoch in range(15):\n",
    "            scheduler.step()\n",
    "            model.train()\n",
    "            for x, y in training_loader:\n",
    "                iteration += 1\n",
    "                x = x.cuda()\n",
    "                y = y.type(torch.FloatTensor).cuda().view(-1, 1)\n",
    "\n",
    "                pred, feat = model(x)\n",
    "                loss = loss_fn(pred, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_predicts = []\n",
    "            val_feats = []\n",
    "            with torch.no_grad():\n",
    "                for x, y in validation_loader:\n",
    "                    x = x.cuda()\n",
    "                    y = y.type(torch.FloatTensor).cuda()#.view(-1, 1)\n",
    "                    v_pred, feat = model(x)\n",
    "                    val_predicts.append(v_pred.cpu().numpy())\n",
    "                    val_feats.append(feat.cpu().numpy())\n",
    "\n",
    "            val_predicts = np.concatenate(val_predicts) \n",
    "            val_feats = np.vstack(val_feats)\n",
    "            pred_test_y_k = getTestScore2(val_predicts.flatten(), tr_cdf)\n",
    "            qwk = quadratic_weighted_kappa(y_val, pred_test_y_k)\n",
    "            val_loss = rmse(y_val,val_predicts)\n",
    "\n",
    "            if val_loss<min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                oof_train_img[valid_idx,:] = np.hstack([val_feats,val_predicts])\n",
    "                test_feats = []\n",
    "                test_preds = []\n",
    "                with torch.no_grad():\n",
    "                    for x, in test_loader:\n",
    "                        x = x.cuda()\n",
    "                        v_pred, feat = model(x)\n",
    "                        test_preds.append(v_pred.cpu().numpy())\n",
    "                        test_feats.append(feat.cpu().numpy())\n",
    "                test_feats = np.hstack([np.vstack(test_feats), np.concatenate(test_preds)])\n",
    "                print(epoch, \"best loss! val loss:\", val_loss, 'qwk:', qwk, \"elapsed time:\", time.time()-since)\n",
    "        oof_test_img.append(test_feats)\n",
    "        rmses.append(min_val_loss)\n",
    "        qwks.append(qwk)\n",
    "        del model\n",
    "        del x\n",
    "        del y\n",
    "        del tra_dataset\n",
    "        del val_dataset\n",
    "        del training_loader\n",
    "        del validation_loader\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    print('overall rmse: %.5f'%rmse(oof_train_img[:,-1], train['AdoptionSpeed']))\n",
    "    print('mean rmse =', np.mean(rmses), 'rmse std =', np.std(rmses))\n",
    "    print('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))\n",
    "    del test_loader\n",
    "    del test_dataset\n",
    "    gc.collect()\n",
    "    return oof_train_img, np.mean(oof_test_img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:43:22.528630Z",
     "start_time": "2019-03-28T10:42:31.083005Z"
    }
   },
   "outputs": [],
   "source": [
    "res34_array = res34_feature_df.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1).values\n",
    "# raw_img_array  = stdscaler.fit_transform(raw_img_array)\n",
    "res34_tra = res34_array[0:train_len]\n",
    "res34_test = res34_array[train_len:]\n",
    "oof_train_res34, oof_test_res34 = get_image_feature(res34_tra, res34_test, IMG_FEATURE_DIM_NN2)\n",
    "del res34_tra\n",
    "del res34_test\n",
    "gc.collect()\n",
    "dnn_resnet34_features = np.vstack([oof_train_res34, oof_test_res34])\n",
    "dnn_resnet34_features = pd.DataFrame(dnn_resnet34_features, columns=['res34_%d'%c for c in range(dnn_resnet34_features.shape[1])])\n",
    "dnn_resnet34_features['PetID'] = data_df['PetID'].values\n",
    "del oof_train_res34\n",
    "del oof_test_res34\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:44:29.611130Z",
     "start_time": "2019-03-28T10:43:22.531996Z"
    }
   },
   "outputs": [],
   "source": [
    "stdscaler = RobustScaler()\n",
    "res50_array = resnet50_feature_df.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1).values\n",
    "# raw_img_array  = stdscaler.fit_transform(raw_img_array)\n",
    "res50_tra = res50_array[0:train_len]\n",
    "res50_test = res50_array[train_len:]\n",
    "oof_train_res, oof_test_res = get_image_feature(res50_tra, res50_test, IMG_FEATURE_DIM_NN)\n",
    "del res50_tra\n",
    "del res50_test\n",
    "gc.collect()\n",
    "dnn_resnet50_features = np.vstack([oof_train_res, oof_test_res])\n",
    "dnn_resnet50_features = pd.DataFrame(dnn_resnet50_features, columns=['resnet_%d'%c for c in range(dnn_resnet50_features.shape[1])])\n",
    "dnn_resnet50_features['PetID'] = data_df['PetID'].values\n",
    "del oof_train_res\n",
    "del oof_test_res\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:45:26.777283Z",
     "start_time": "2019-03-28T10:44:29.612724Z"
    }
   },
   "outputs": [],
   "source": [
    "dense121_array = dense121_feature_df.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1).values\n",
    "dense121_tra = dense121_array[0:train_len]\n",
    "dense121_test = dense121_array[train_len:]\n",
    "oof_train_dense121, oof_test_dense121 = get_image_feature(dense121_tra, dense121_test, IMG_FEATURE_DIM_NN)\n",
    "del dense121_tra\n",
    "del dense121_test\n",
    "gc.collect()\n",
    "dnn_dense121_features = np.vstack([oof_train_dense121, oof_test_dense121])\n",
    "dnn_dense121_features = pd.DataFrame(dnn_dense121_features, columns=['dense121_%d'%c for c in range(dnn_dense121_features.shape[1])])\n",
    "dnn_dense121_features['PetID'] = data_df['PetID'].values\n",
    "del oof_train_dense121\n",
    "del oof_test_dense121\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:46:33.161537Z",
     "start_time": "2019-03-28T10:45:26.782300Z"
    }
   },
   "outputs": [],
   "source": [
    "dense_array = dense_feature_df.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1).values\n",
    "dense_tra = dense_array[0:train_len]\n",
    "dense_test = dense_array[train_len:]\n",
    "oof_train_dense, oof_test_dense = get_image_feature(dense_tra, dense_test, IMG_FEATURE_DIM_NN)\n",
    "del dense_tra\n",
    "del dense_test\n",
    "gc.collect()\n",
    "dnn_dense_features = np.vstack([oof_train_dense, oof_test_dense])\n",
    "dnn_dense_features = pd.DataFrame(dnn_dense_features, columns=['dense_%d'%c for c in range(dnn_dense_features.shape[1])])\n",
    "dnn_dense_features['PetID'] = data_df['PetID'].values\n",
    "del oof_train_dense\n",
    "del oof_test_dense\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:38:49.684009Z",
     "start_time": "2019-03-28T12:38:48.807173Z"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 64\n",
    "\n",
    "\n",
    "# features = raw_img_features.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1).values\n",
    "svd_ = TruncatedSVD(n_components=32, random_state=1337)\n",
    "svd_resnet50_features = svd_.fit_transform(res50_array)\n",
    "svd_resnet50_features = pd.DataFrame(svd_resnet50_features)\n",
    "svd_resnet50_features = svd_resnet50_features.add_prefix('resnet50_SVD_')\n",
    "svd_resnet50_features['PetID'] = resnet50_feature_df['PetID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:38:50.146570Z",
     "start_time": "2019-03-28T12:38:49.686718Z"
    }
   },
   "outputs": [],
   "source": [
    "svd_ = TruncatedSVD(n_components=32, random_state=1337)\n",
    "svd_dense121_features = svd_.fit_transform(dense121_array)\n",
    "svd_dense121_features = pd.DataFrame(svd_dense121_features)\n",
    "svd_dense121_features = svd_dense121_features.add_prefix('dense121_SVD_')\n",
    "svd_dense121_features['PetID'] = resnet50_feature_df['PetID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:38:50.934069Z",
     "start_time": "2019-03-28T12:38:50.148784Z"
    }
   },
   "outputs": [],
   "source": [
    "svd_ = TruncatedSVD(n_components=32, random_state=1337)\n",
    "svd_dense_features = svd_.fit_transform(dense_array)\n",
    "svd_dense_features = pd.DataFrame(svd_dense_features)\n",
    "svd_dense_features = svd_dense_features.add_prefix('dense_SVD_')\n",
    "svd_dense_features['PetID'] = dense_feature_df['PetID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:46:35.746303Z",
     "start_time": "2019-03-28T10:46:33.173612Z"
    }
   },
   "outputs": [],
   "source": [
    "svd_ = TruncatedSVD(n_components=64, random_state=1337)\n",
    "svd_img_features = svd_.fit_transform(np.hstack([res50_array, dense_array, dense121_array, res34_array]))\n",
    "svd_img_features = pd.DataFrame(svd_img_features)\n",
    "svd_img_features = svd_img_features.add_prefix('IMG_SVD_')\n",
    "svd_img_features['PetID'] = resnet50_feature_df['PetID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:46:40.759394Z",
     "start_time": "2019-03-28T10:46:35.747775Z"
    }
   },
   "outputs": [],
   "source": [
    "#img clustering\n",
    "from sklearn.cluster import DBSCAN, FeatureAgglomeration, KMeans\n",
    "\n",
    "features = svd_img_features.drop(['PetID'], axis=1).values\n",
    "\n",
    "cluster = KMeans(n_clusters=32)\n",
    "cluster_label = cluster.fit_predict(features)\n",
    "\n",
    "cluster_img_features = pd.DataFrame(cluster_label)\n",
    "cluster_img_features = cluster_img_features.add_prefix('img_CLUSTER_')\n",
    "cluster_img_features['PetID'] = dense_feature_df['PetID'].values\n",
    "\n",
    "# features = svd_dense_features.drop(['PetID'], axis=1).values\n",
    "\n",
    "# cluster = KMeans(n_clusters=32)\n",
    "# cluster_label = cluster.fit_predict(features)\n",
    "\n",
    "# cluster_dense_features = pd.DataFrame(cluster_label)\n",
    "# cluster_dense_features = cluster_dense_features.add_prefix('dense_CLUSTER_')\n",
    "# cluster_dense_features['PetID'] = dense_feature_df['PetID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:46:40.780589Z",
     "start_time": "2019-03-28T10:46:40.760993Z"
    }
   },
   "outputs": [],
   "source": [
    "svd_img_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:46:40.785450Z",
     "start_time": "2019-03-28T10:46:40.782753Z"
    }
   },
   "outputs": [],
   "source": [
    "# svd_resnet50_features.to_pickle('image_features_svd.pkl')\n",
    "# dnn_resnet50_features.to_pickle('image_features_nn.pkl')\n",
    "# cluster_resnet50_features.to_pickle('image_features_cluster.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18f435d349d1fc384c645b41233d2c5a22860903"
   },
   "source": [
    "## meta & senti feature\n",
    "\n",
    "After taking a look at the data, we know its structure and can use it to extract additional features and concatenate them with basic train/test DFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:46:40.834289Z",
     "start_time": "2019-03-28T10:46:40.787135Z"
    },
    "_kg_hide-input": true,
    "_uuid": "1db107527d3ef5ba55560679334d16f1d82fb663",
    "code_folding": [
     0,
     8,
     16,
     24,
     31,
     59,
     70,
     91
    ]
   },
   "outputs": [],
   "source": [
    "class PetFinderParser(object):\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "        self.sentence_sep = '; '\n",
    "        \n",
    "        # Does not have to be extracted because main DF already contains description\n",
    "        self.extract_sentiment_text = False\n",
    "        \n",
    "    def open_metadata_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load metadata file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f:\n",
    "            metadata_file = json.load(f)\n",
    "        return metadata_file\n",
    "            \n",
    "    def open_sentiment_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load sentiment file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f:\n",
    "            sentiment_file = json.load(f)\n",
    "        return sentiment_file\n",
    "            \n",
    "    def open_image_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load image file.\n",
    "        \"\"\"\n",
    "        image = np.asarray(Image.open(filename))\n",
    "        return image\n",
    "        \n",
    "    def parse_sentiment_file(self, file):\n",
    "        \"\"\"\n",
    "        Parse sentiment file. Output DF with sentiment features.\n",
    "        \"\"\"\n",
    "        # documentSentiment\n",
    "        ret_val = {}\n",
    "        ret_val['doc_mag'] = file['documentSentiment']['magnitude']\n",
    "        ret_val['doc_score']= file['documentSentiment']['score']\n",
    "        ret_val['doc_language'] = file['language']\n",
    "        ret_val['doc_stcs_len'] = len(file['sentences'])\n",
    "        if ret_val['doc_stcs_len']>0:\n",
    "            ret_val['doc_first_score'] = file['sentences'][0]['sentiment']['score']\n",
    "            ret_val['doc_first_mag'] = file['sentences'][0]['sentiment']['magnitude']\n",
    "            ret_val['doc_last_score'] = file['sentences'][-1]['sentiment']['score']\n",
    "            ret_val['doc_last_mag'] = file['sentences'][-1]['sentiment']['magnitude']\n",
    "        else:\n",
    "            ret_val['doc_first_score'] = np.nan\n",
    "            ret_val['doc_first_mag'] = np.nan\n",
    "            ret_val['doc_last_score'] = np.nan\n",
    "            ret_val['doc_last_mag'] = np.nan\n",
    "        ret_val['doc_ent_num'] = len(file['entities'])\n",
    "        \n",
    "        # sentence score\n",
    "        mags, scores = [], []\n",
    "        for s in file['sentences']:\n",
    "            mags.append(s['sentiment']['magnitude'])\n",
    "            scores.append(s['sentiment']['score'])\n",
    "        \n",
    "        if len(scores)==0:\n",
    "            ret_val['doc_score_sum'] = np.nan\n",
    "            ret_val['doc_mag_sum'] = np.nan\n",
    "            ret_val['doc_score_mena'] = np.nan\n",
    "            ret_val['doc_mag_mean'] = np.nan\n",
    "            ret_val['doc_score_max'] = np.nan\n",
    "            ret_val['doc_mag_max'] = np.nan\n",
    "            ret_val['doc_score_min'] = np.nan\n",
    "            ret_val['doc_mag_min'] = np.nan\n",
    "            ret_val['doc_score_std'] = np.nan\n",
    "            ret_val['doc_mag_std'] = np.nan\n",
    "        else:\n",
    "            ret_val['doc_score_sum'] = np.sum(scores)\n",
    "            ret_val['doc_mag_sum'] = np.sum(mags)\n",
    "            ret_val['doc_score_mena'] = np.mean(scores)\n",
    "            ret_val['doc_mag_mean'] = np.mean(mags)\n",
    "            ret_val['doc_score_max'] = np.max(scores)\n",
    "            ret_val['doc_mag_max'] = np.max(mags)\n",
    "            ret_val['doc_score_min'] = np.min(scores)\n",
    "            ret_val['doc_mag_min'] = np.min(mags)\n",
    "            ret_val['doc_score_std'] = np.std(scores)\n",
    "            ret_val['doc_mag_std'] = np.std(mags)\n",
    "\n",
    "        # entity type\n",
    "        ret_val['sentiment_entities'] = []\n",
    "        ret_val['doc_ent_person_count'] = 0\n",
    "        ret_val['doc_ent_location_count'] = 0\n",
    "        ret_val['doc_ent_org_count'] = 0\n",
    "        ret_val['doc_ent_event_count'] = 0\n",
    "        ret_val['doc_ent_woa_count'] = 0\n",
    "        ret_val['doc_ent_good_count'] = 0\n",
    "        ret_val['doc_ent_other_count'] = 0\n",
    "        key_mapper = {\n",
    "            'PERSON':'doc_ent_person_count',\n",
    "            'LOCATION':'doc_ent_location_count',\n",
    "            'ORGANIZATION':'doc_ent_org_count',\n",
    "            'EVENT':'doc_ent_event_count',\n",
    "            'WORK_OF_ART':'doc_ent_woa_count',\n",
    "            'CONSUMER_GOOD':'doc_ent_good_count',\n",
    "            'OTHER':'doc_ent_other_count'\n",
    "        }\n",
    "        for e in file['entities']:\n",
    "            ret_val['sentiment_entities'].append(e['name'])\n",
    "            if e['type'] in key_mapper:\n",
    "                ret_val[key_mapper[e['type']]]+=1\n",
    "        \n",
    "        ret_val['sentiment_entities'] = ' '.join(ret_val['sentiment_entities'])\n",
    "        return ret_val\n",
    "    \n",
    "    def parse_metadata_file(self, file, img):\n",
    "        \"\"\"\n",
    "        Parse metadata file. Output DF with metadata features.\n",
    "        \"\"\"\n",
    "        file_keys = list(file.keys())\n",
    "        if 'textAnnotations' in file_keys:\n",
    "#             textanno = 1\n",
    "            textblock_num = len(file['textAnnotations'])\n",
    "            textlen = np.sum([len(text['description']) for text in file['textAnnotations']])\n",
    "        else:\n",
    "#             textanno = 0\n",
    "            textblock_num = 0\n",
    "            textlen = 0\n",
    "        if 'faceAnnotations' in file_keys:\n",
    "            faceanno = 1\n",
    "        else:\n",
    "            faceanno = 0\n",
    "        if 'labelAnnotations' in file_keys:\n",
    "            file_annots = file['labelAnnotations']#[:len(file['labelAnnotations'])]\n",
    "            if len(file_annots)==0:\n",
    "                file_label_score_mean = np.nan\n",
    "                file_label_score_max = np.nan\n",
    "                file_label_score_min = np.nan\n",
    "            else:\n",
    "                temp = np.asarray([x['score'] for x in file_annots])\n",
    "                file_label_score_mean = temp.mean()\n",
    "                file_label_score_max = temp.max()\n",
    "                file_label_score_min = temp.min()\n",
    "            file_top_desc = [x['description'] for x in file_annots]\n",
    "        else:\n",
    "            file_label_score_mean = np.nan\n",
    "            file_label_score_max = np.nan\n",
    "            file_label_score_min = np.nan\n",
    "            file_top_desc = ['']\n",
    "        \n",
    "        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "        file_crops = file['cropHintsAnnotation']['cropHints']\n",
    "        if len(file_colors)==0:\n",
    "            file_color_score = np.nan\n",
    "            file_color_pixelfrac = np.nan\n",
    "            color_red_mean = np.nan\n",
    "            color_green_mean = np.nan\n",
    "            color_blue_mean = np.nan\n",
    "            color_red_std = np.nan\n",
    "            color_green_std = np.nan\n",
    "            color_blue_std = np.nan\n",
    "        else:\n",
    "            file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "            file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "            file_color_red = np.asarray([x['color']['red'] if 'red' in x['color'] else 0 for x in file_colors])\n",
    "            file_color_green = np.asarray([x['color']['green'] if 'green' in x['color'] else 0for x in file_colors])\n",
    "            file_color_blue = np.asarray([x['color']['blue'] if 'blue' in x['color'] else 0 for x in file_colors])\n",
    "            color_red_mean = file_color_red.mean()\n",
    "            color_green_mean = file_color_green.mean()\n",
    "            color_blue_mean = file_color_blue.mean()\n",
    "            color_red_std = file_color_red.std()\n",
    "            color_green_std = file_color_green.std()\n",
    "            color_blue_std = file_color_blue.std()\n",
    "        \n",
    "        if len(file_crops)==0:\n",
    "            file_crop_conf=np.nan\n",
    "            file_crop_importance = np.nan\n",
    "            file_crop_fraction_mean = np.nan\n",
    "            file_crop_fraction_sum = np.nan\n",
    "            file_crop_fraction_std = np.nan\n",
    "            file_crop_num = 0\n",
    "        else:\n",
    "            file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n",
    "            file_crop_num = len(file_crops)\n",
    "            if 'importanceFraction' in file_crops[0].keys():\n",
    "                file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n",
    "            else:\n",
    "                file_crop_importance = np.nan\n",
    "            crop_areas = []\n",
    "            image_area = img.shape[0]*img.shape[1]\n",
    "            for crophint in file_crops:\n",
    "                v_x, v_y = [], []\n",
    "                for vertices in crophint['boundingPoly']['vertices']:\n",
    "                    if 'x' not in vertices:\n",
    "                        v_x.append(0)\n",
    "                    else:\n",
    "                        v_x.append(vertices['x'])\n",
    "                    if 'y' not in vertices:\n",
    "                        v_y.append(0)\n",
    "                    else:\n",
    "                        v_y.append(vertices['y'])\n",
    "                crop_areas.append((max(v_x)-min(v_x))*(max(v_y)-min(v_y))/image_area)\n",
    "            file_crop_fraction_mean = np.mean(crop_areas)\n",
    "            file_crop_fraction_sum = np.sum(crop_areas)\n",
    "            file_crop_fraction_std = np.std(crop_areas)\n",
    "\n",
    "        df_metadata = {\n",
    "            'label_score_mean': file_label_score_mean,\n",
    "            'label_score_max': file_label_score_max,\n",
    "            'label_score_min': file_label_score_min,\n",
    "            'color_score': file_color_score,\n",
    "            'color_pixelfrac': file_color_pixelfrac,\n",
    "            'crop_conf': file_crop_conf,\n",
    "            'crop_importance': file_crop_importance,\n",
    "            'color_red_mean':color_red_mean,\n",
    "            'color_green_mean':color_green_mean,\n",
    "            'color_blue_mean':color_blue_mean,\n",
    "            'color_red_std':color_red_std,\n",
    "            'color_green_std':color_green_std,\n",
    "            'color_blue_std':color_blue_std,\n",
    "#             'crop_area_mean':file_crop_fraction_mean,\n",
    "            'crop_area_sum':file_crop_fraction_sum,\n",
    "#             'crop_area_std':file_crop_fraction_std,\n",
    "            'annots_top_desc': self.sentence_sep.join(file_top_desc),\n",
    "            'img_aratio':img.shape[0]/img.shape[1],\n",
    "#             'text_annotation':textanno,\n",
    "            'text_len':textlen,\n",
    "            'textblock_num':textblock_num,\n",
    "            'face_annotation':faceanno\n",
    "        }\n",
    "        \n",
    "        return df_metadata\n",
    "    \n",
    "# Helper function for parallel data processing:\n",
    "def extract_additional_features(pet_id, mode='train'):\n",
    "    sentiment_filename = '../input/petfinder-adoption-prediction/{}_sentiment/{}.json'.format(mode, pet_id)\n",
    "    try:\n",
    "        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n",
    "        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n",
    "        df_sentiment['PetID'] = pet_id\n",
    "    except FileNotFoundError:\n",
    "        df_sentiment = None\n",
    "\n",
    "    dfs_metadata = []\n",
    "    for ind in range(1,200):\n",
    "        metadata_filename = '../input/petfinder-adoption-prediction/{}_metadata/{}-{}.json'.format(mode, pet_id, ind)\n",
    "        image_filename = '../input/petfinder-adoption-prediction/{}_images/{}-{}.jpg'.format(mode, pet_id, ind)\n",
    "        try:\n",
    "            image = cv2.imread(image_filename)\n",
    "            metadata_file = pet_parser.open_metadata_file(metadata_filename)\n",
    "            df_metadata = pet_parser.parse_metadata_file(metadata_file, image)\n",
    "            df_metadata['PetID'] = pet_id\n",
    "            dfs_metadata.append(df_metadata)\n",
    "        except FileNotFoundError:\n",
    "            break\n",
    "    return [df_sentiment, dfs_metadata]\n",
    "    \n",
    "pet_parser = PetFinderParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:19.652455Z",
     "start_time": "2019-03-28T10:46:40.836476Z"
    },
    "_uuid": "1b8c6eedb089ac6df143592de6455c1978b460f6",
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Unique IDs from train and test:\n",
    "train_pet_ids = train.PetID.unique()\n",
    "test_pet_ids = test.PetID.unique()\n",
    "n_jobs = 8\n",
    "\n",
    "# Train set:\n",
    "# Parallel processing of data:\n",
    "dfs_train = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n",
    "\n",
    "# Extract processed data and format them as DFs:\n",
    "train_dicts_sentiment = [x[0] for x in dfs_train if x[0] is not None]\n",
    "train_dfs_metadata = [x[1] for x in dfs_train if len([x[1]])>0]\n",
    "\n",
    "train_dfs_sentiment = pd.DataFrame(train_dicts_sentiment)\n",
    "train_dfs_metadata = list(itertools.chain.from_iterable(train_dfs_metadata))\n",
    "train_dfs_metadata = pd.DataFrame(train_dfs_metadata)\n",
    "\n",
    "print(train_dfs_sentiment.shape, train_dfs_metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:29.319803Z",
     "start_time": "2019-03-28T10:47:19.654920Z"
    },
    "_uuid": "1b8c6eedb089ac6df143592de6455c1978b460f6",
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test set:\n",
    "# Parallel processing of data:\n",
    "dfs_test = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n",
    "\n",
    "# Extract processed data and format them as DFs:\n",
    "test_dicts_sentiment = [x[0] for x in dfs_test if x[0] is not None]\n",
    "test_dfs_metadata = [x[1] for x in dfs_test if len(x[1])>0]\n",
    "\n",
    "test_dfs_sentiment = pd.DataFrame(test_dicts_sentiment)\n",
    "test_dfs_metadata = list(itertools.chain.from_iterable(test_dfs_metadata))\n",
    "test_dfs_metadata = pd.DataFrame(test_dfs_metadata)\n",
    "\n",
    "print(test_dfs_sentiment.shape, test_dfs_metadata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "76489c2957389009c1b55ec2142ebf5dd9d7a923"
   },
   "source": [
    "### group extracted features by PetID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:29.380217Z",
     "start_time": "2019-03-28T10:47:29.321557Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_df = pd.concat([train_dfs_metadata, test_dfs_metadata], sort=False).reset_index(drop=True)\n",
    "senti_df = pd.concat([train_dfs_sentiment, test_dfs_sentiment], sort=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:29.465447Z",
     "start_time": "2019-03-28T10:47:29.382413Z"
    }
   },
   "outputs": [],
   "source": [
    "# meta_df.to_pickle('./meta_df.pkl')\n",
    "# senti_df.to_pickle('./senti_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:29.469136Z",
     "start_time": "2019-03-28T10:47:29.467138Z"
    }
   },
   "outputs": [],
   "source": [
    "# meta_df = pd.read_pickle('./meta_df.pkl')\n",
    "# senti_df = pd.read_pickle('./senti_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:36.188617Z",
     "start_time": "2019-03-28T10:47:29.470914Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata_desc = meta_df.groupby(['PetID'])['annots_top_desc'].unique().reset_index()\n",
    "metadata_desc['meta_annots_top_desc'] = metadata_desc['annots_top_desc'].apply(lambda x: '; '.join(x))\n",
    "metadata_desc.drop('annots_top_desc', axis=1, inplace=True)\n",
    "\n",
    "possible_annots = set()\n",
    "for i in range(len(meta_df)):\n",
    "    possible_annots = possible_annots.union(set(meta_df['annots_top_desc'].iloc[i].split('; ')))\n",
    "annot_mapper = {}\n",
    "for idx, a in enumerate(possible_annots):\n",
    "    annot_mapper[a] = str(idx)\n",
    "metadata_desc['meta_desc'] = metadata_desc['meta_annots_top_desc'].apply(lambda x: ' '.join(annot_mapper[i] for i in x.split('; ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:36.299676Z",
     "start_time": "2019-03-28T10:47:36.210680Z"
    }
   },
   "outputs": [],
   "source": [
    "# sentiment feature\n",
    "senti_df['sentiment_entities'].fillna('', inplace=True)\n",
    "senti_df['sentiment_entities'] = senti_df['sentiment_entities'].str.lower()\n",
    "senti_df['sentiment_len'] = senti_df['sentiment_entities'].apply(lambda x:len(x))\n",
    "senti_df['sentiment_word_len'] = senti_df['sentiment_entities'].apply(lambda x: len(x.replace(';',' ').split(' ')))\n",
    "senti_df['sentiment_word_unique'] = senti_df['sentiment_entities'].apply(lambda x: len(set(x.replace(';',' ').split(' '))))\n",
    "\n",
    "senti_df['doc_language'] = pd.factorize(senti_df['doc_language'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:36.497874Z",
     "start_time": "2019-03-28T10:47:36.311474Z"
    },
    "_uuid": "558824a494ce6b6c5821d2cb85420393bf728338",
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# meta agg\n",
    "aggregates = {\n",
    "    'color_blue_mean':['mean','std'],\n",
    "    'color_blue_std':['mean'],\n",
    "    'color_green_mean':['mean','std'], \n",
    "    'color_green_std':['mean'],\n",
    "    'color_pixelfrac':['mean','std'],\n",
    "    'color_red_mean':['mean','std'],\n",
    "    'color_red_std':['mean'],\n",
    "    'color_score':['mean','max'], \n",
    "#     'crop_area_mean':['mean','std','max'],\n",
    "#     'crop_area_std':['mean'], \n",
    "    'crop_area_sum':['mean','std','min'], \n",
    "    'crop_conf':['mean','std','max'],\n",
    "    'crop_importance':['mean','std'],\n",
    "    'label_score_max':['mean','std','max'],\n",
    "    'label_score_mean':['mean','max','std'],\n",
    "    'label_score_min':['mean','max','std'],\n",
    "    'img_aratio':['nunique','std','max','min'],\n",
    "    'textblock_num':['mean','max'],\n",
    "#     'text_len':['mean','max'],\n",
    "    'face_annotation':['mean','nunique']\n",
    "}\n",
    "\n",
    "# Train\n",
    "metadata_gr = meta_df.drop(['annots_top_desc'], axis=1)\n",
    "for i in metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        metadata_gr[i] = metadata_gr[i].astype(float)\n",
    "metadata_gr = metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "metadata_gr.columns = pd.Index(['{}_{}_{}'.format('meta', c[0], c[1].upper()) for c in metadata_gr.columns.tolist()])\n",
    "metadata_gr = metadata_gr.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:36.520971Z",
     "start_time": "2019-03-28T10:47:36.499399Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_df = metadata_desc.merge(metadata_gr, on='PetID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:36.528066Z",
     "start_time": "2019-03-28T10:47:36.522568Z"
    }
   },
   "outputs": [],
   "source": [
    "# annotation feature\n",
    "meta_df['meta_annots_top_desc'].fillna(' ', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:36.560749Z",
     "start_time": "2019-03-28T10:47:36.529560Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_df[meta_df['meta_textblock_num_MEAN']>0.8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a424da889496d55188c3c8478f0035c9fa8a7555",
    "heading_collapsed": true
   },
   "source": [
    "## feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:36.566701Z",
     "start_time": "2019-03-28T10:47:36.562203Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feat_df = data_df[['PetID','Color1','Breed1','State','RescuerID','Name','Breed_full','Color_full','hard_interaction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:36.634141Z",
     "start_time": "2019-03-28T10:47:36.568235Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# color feature\n",
    "agg = {\n",
    "    'Fee':['mean','std','max'],\n",
    "    'avg_fee':['mean','std','max'],\n",
    "    'Breed1':['nunique'],\n",
    "    #'Gender':['nunique'],\n",
    "    'Age':['mean','std','max'], #,'min'\n",
    "    'Quantity':['std'],#'mean',,'min','max'\n",
    "    'PetID':['nunique']\n",
    "}\n",
    "feat = data_df.groupby('Color1').agg(agg)\n",
    "feat.columns = pd.Index(['COLOR_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\n",
    "feat.reset_index(inplace=True)\n",
    "feat_df = feat_df.merge(feat, on='Color1', how='left')\n",
    "\n",
    "agg = {\n",
    "    'Fee':['mean','std','max'],\n",
    "    'avg_fee':['mean','std','max'],\n",
    "    'Breed_full':['nunique'],\n",
    "    'Quantity':['sum'],\n",
    "}\n",
    "feat = data_df.groupby('Color_full').agg(agg)\n",
    "feat.columns = pd.Index(['COLORfull_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\n",
    "feat.reset_index(inplace=True)\n",
    "feat_df = feat_df.merge(feat, on='Color_full', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:36.768747Z",
     "start_time": "2019-03-28T10:47:36.635806Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Breed feature\n",
    "agg = {\n",
    "    'Color_full':['nunique'],\n",
    "    'Breed2':['nunique'],\n",
    "    'FurLength':['nunique'],\n",
    "    'Fee':['mean','max'],#,'min'\n",
    "    'avg_fee':['mean','std','max'],\n",
    "    'Age':['mean','std','min','max'],\n",
    "    'Quantity':['mean','std','max','sum'],#'min'\n",
    "    'PetID':['nunique'],\n",
    "    'FurLength':['mean'],\n",
    "    'Health':['mean'],\n",
    "    'MaturitySize':['mean','std','min','max'],\n",
    "    'Vaccinated':['mean'],\n",
    "    'Dewormed':['mean'],\n",
    "    'Sterilized':['mean']\n",
    "}\n",
    "feat = data_df.groupby('Breed1').agg(agg)\n",
    "feat.columns = pd.Index(['BREED1_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\n",
    "feat.reset_index(inplace=True)\n",
    "feat_df = feat_df.merge(feat, on='Breed1', how='left')\n",
    "\n",
    "# Breed feature\n",
    "agg = {\n",
    "    'Color_full':['nunique'],\n",
    "    'Fee':['mean','min','max'],\n",
    "    'avg_fee':['mean','std','max'],\n",
    "    'Quantity':['sum'],\n",
    "    'PetID':['nunique']\n",
    "}\n",
    "feat = data_df.groupby('Breed_full').agg(agg)\n",
    "feat.columns = pd.Index(['BREEDfull_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\n",
    "feat.reset_index(inplace=True)\n",
    "feat_df = feat_df.merge(feat, on='Breed_full', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:36.854999Z",
     "start_time": "2019-03-28T10:47:36.770414Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# State feature\n",
    "agg = {\n",
    "    'Color_full':['nunique'],\n",
    "    'Breed_full':['nunique'],\n",
    "    'PetID':['nunique'],\n",
    "    'RescuerID':['nunique'],\n",
    "    'Fee':['mean','max'],\n",
    "    'avg_fee':['mean','std','max'],\n",
    "    'Age':['mean','std','max'],\n",
    "    'Quantity':['mean','std','max'],#,'min','sum'\n",
    "    'FurLength':['mean','std'],\n",
    "    'Health':['mean'],\n",
    "    'MaturitySize':['mean','std'],\n",
    "    'Vaccinated':['mean'],\n",
    "    'Dewormed':['mean'],\n",
    "    'Sterilized':['mean'],\n",
    "    'VideoAmt':['mean','std'],\n",
    "    'PhotoAmt':['mean','std'],\n",
    "    'avg_photo':['mean','std']\n",
    "}\n",
    "feat = data_df.groupby('State').agg(agg)\n",
    "feat.columns = pd.Index(['STATE_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\n",
    "feat.reset_index(inplace=True)\n",
    "feat_df = feat_df.merge(feat, on='State', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:36.976175Z",
     "start_time": "2019-03-28T10:47:36.856545Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# multiple agg\n",
    "agg = {\n",
    "    'Fee':['mean','min','max'],\n",
    "    'avg_fee':['mean','min','max'],\n",
    "    'Age':['mean','std','min','max'],\n",
    "    'Quantity':['mean','std','sum'],\n",
    "    'PetID':['nunique'],\n",
    "    'FurLength':['mean'],\n",
    "    'Health':['mean'],\n",
    "    'MaturitySize':['mean','std'],\n",
    "    'Vaccinated':['mean'],\n",
    "    'Dewormed':['mean'],\n",
    "    'Sterilized':['mean']\n",
    "}\n",
    "feat = data_df.groupby(['State','Breed1','Color1']).agg(agg)\n",
    "feat.columns = pd.Index(['MULTI_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\n",
    "feat.reset_index(inplace=True)\n",
    "feat_df = feat_df.merge(feat, on=['State','Breed1','Color1'], how='left')\n",
    "\n",
    "agg = {\n",
    "    'Fee':['mean','min','max'],\n",
    "    'avg_fee':['mean','min','max'],\n",
    "    'Age':['mean','std','min','max'],\n",
    "    'Quantity':['mean','std','sum'],\n",
    "    'PetID':['nunique'],\n",
    "}\n",
    "feat = data_df.groupby(['State','Breed_full','Color_full']).agg(agg)\n",
    "feat.columns = pd.Index(['MULTI2_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\n",
    "feat.reset_index(inplace=True)\n",
    "feat_df = feat_df.merge(feat, on=['State','Breed_full','Color_full'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:37.029082Z",
     "start_time": "2019-03-28T10:47:36.977944Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# name feature\n",
    "feat = data_df.groupby('Name')['PetID'].agg({'name_count':'nunique'}).reset_index()\n",
    "feat_df = feat_df.merge(feat, on='Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:37.102752Z",
     "start_time": "2019-03-28T10:47:37.030797Z"
    },
    "_uuid": "68d8f68c17adb43e1c5c86a773058b09c208d721",
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count RescuerID occurrences:\n",
    "agg = {\n",
    "#     'avg_fee':['mean','std'], # hurt\n",
    "#     'Age':['mean','std'], #,'min','max' hurt\n",
    "#     'Quantity':['mean','std','sum'],\n",
    "    'PetID':['nunique'],\n",
    "    'Breed_full':['nunique'],\n",
    "    'VideoAmt':['mean','std'],\n",
    "    'PhotoAmt':['mean','std'],\n",
    "    'avg_photo':['mean','std'],\n",
    "    'Sterilized':['mean'],\n",
    "    'Dewormed':['mean'],\n",
    "    'Vaccinated':['mean']\n",
    "#     'description_word_len':['mean','std'] # hurt\n",
    "}\n",
    "rescuer_count = data_df.groupby(['RescuerID']).agg(agg)\n",
    "rescuer_count.columns = pd.Index(['RESCUER_' + e[0] + \"_\" + e[1].upper() for e in rescuer_count.columns.tolist()])\n",
    "rescuer_count.reset_index(inplace=True)\n",
    "feat_df = feat_df.merge(rescuer_count, how='left', on='RescuerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:37.147209Z",
     "start_time": "2019-03-28T10:47:37.104282Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# State feature\n",
    "agg = {\n",
    "    'Fee':['mean','min','max'],\n",
    "    'avg_fee':['mean','std','max']\n",
    "}\n",
    "feat = data_df.groupby('hard_interaction').agg(agg)\n",
    "feat.columns = pd.Index(['INTERACTION_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\n",
    "feat.reset_index(inplace=True)\n",
    "feat_df = feat_df.merge(feat, on='hard_interaction', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:37.172915Z",
     "start_time": "2019-03-28T10:47:37.148924Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feat_df.drop(['Color1','Breed1','State','RescuerID','Name','Breed_full','Color_full','hard_interaction'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:47:37.255893Z",
     "start_time": "2019-03-28T10:47:37.175058Z"
    }
   },
   "outputs": [],
   "source": [
    "X_text = data_df[['PetID','Description']].merge(data_df[['PetID','Chinese_desc']], on='PetID', how='left')\n",
    "X_text = X_text.merge(senti_df[['PetID','sentiment_entities']], on='PetID', how='left')\n",
    "X_text = X_text.merge(metadata_desc[['PetID','meta_annots_top_desc','meta_desc']], on='PetID', how='left')\n",
    "text_columns = ['Description','Chinese_desc','sentiment_entities','meta_annots_top_desc','meta_desc']\n",
    "print(text_columns)\n",
    "X_text['meta_annots_top_desc'].fillna(' ',inplace=True)\n",
    "X_text['meta_desc'].fillna(' ',inplace=True)\n",
    "X_text['sentiment_entities'].fillna(' ',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:48:17.152954Z",
     "start_time": "2019-03-28T10:47:37.257538Z"
    },
    "_uuid": "0f5d0c82ad86bf1010cda2db2f35c1e32f4406fa",
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_features = []\n",
    "ngram_ranges = [(1,3),(1,3),(1,1),(1,3),(1,1)]\n",
    "n_components = [80, 24, 10, 32,16]\n",
    "\n",
    "# Generate text features:\n",
    "for idx, i in enumerate(text_columns):\n",
    "    # Initialize decomposition methods:\n",
    "    print('generating features from: {}'.format(i))\n",
    "    svd_ = TruncatedSVD(\n",
    "        n_components=n_components[idx], random_state=1337)\n",
    "    \n",
    "    tfidf_col = TfidfVectorizer(ngram_range = ngram_ranges[idx], stop_words = 'english', #lowercase=False,\n",
    "                                tokenizer=custom_tokenizer,\n",
    "                               strip_accents='unicode').fit_transform(X_text.loc[:, i].values)\n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col)\n",
    "    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n",
    "    \n",
    "    text_features.append(svd_col)\n",
    "\n",
    "# Combine all extracted features:\n",
    "# text_features = pd.concat(text_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:48:38.660160Z",
     "start_time": "2019-03-28T10:48:17.154505Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = [custom_tokenizer(x) for x in data_df['Description'].values]\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "name = 'Description'\n",
    "corpus = [dictionary.doc2bow(text) for text in docs]\n",
    "lda = LdaMulticore(corpus, id2word=dictionary, num_topics=20, random_state = 999)\n",
    "docres = [dict(lda[doc_bow]) for doc_bow in corpus]\n",
    "doc_df = pd.DataFrame(docres,dtype=np.float16).fillna(0.001)\n",
    "doc_df.columns = ['%s_lda_%d'%(name,x) for x in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:48:38.665573Z",
     "start_time": "2019-03-28T10:48:38.662300Z"
    }
   },
   "outputs": [],
   "source": [
    "text_features.append(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:48:56.734807Z",
     "start_time": "2019-03-28T10:48:38.667722Z"
    }
   },
   "outputs": [],
   "source": [
    "X_text_char = data_df[['PetID','Name','BreedName_full']].merge(metadata_desc[['PetID','meta_annots_top_desc']], on='PetID', how='left')\n",
    "X_text_char['meta_annots_top_desc'].fillna(' ',inplace=True)\n",
    "for c in ['Name','BreedName_full','meta_annots_top_desc']:\n",
    "    svd_ = TruncatedSVD(\n",
    "        n_components=16, random_state=1337)\n",
    "\n",
    "    tfidf_col = TfidfVectorizer(ngram_range = (1,5), analyzer='char',#lowercase=False,\n",
    "                               strip_accents='unicode').fit_transform(X_text_char[c])\n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col)\n",
    "    svd_col = svd_col.add_prefix('SVD_CHAR_{}_'.format(c))\n",
    "\n",
    "    text_features.append(svd_col)\n",
    "del X_text_char\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:48:56.744856Z",
     "start_time": "2019-03-28T10:48:56.736857Z"
    }
   },
   "outputs": [],
   "source": [
    "text_features = pd.concat(text_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:48:56.751622Z",
     "start_time": "2019-03-28T10:48:56.746878Z"
    }
   },
   "outputs": [],
   "source": [
    "text_features['PetID'] = X_text['PetID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:48:56.818104Z",
     "start_time": "2019-03-28T10:48:56.753942Z"
    }
   },
   "outputs": [],
   "source": [
    "# text_features.to_pickle('text_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:48:56.828882Z",
     "start_time": "2019-03-28T10:48:56.820048Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_lda_feature(data, target, source, name, n_topic = 10, random_state = 999):\n",
    "    retval = pd.DataFrame(data[[target, source]])\n",
    "    x = retval.groupby(target, as_index=False)[source].agg({'list':(lambda x: list(x))})\n",
    "    x['sentence'] = x['list'].apply(lambda x: list(map(str,x)))\n",
    "    docs = x['sentence'].tolist() #.apply(lambda x:x.split()).tolist()\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(text) for text in docs]\n",
    "    lda = LdaMulticore(corpus, id2word=dictionary, num_topics=n_topic, random_state = random_state)\n",
    "    docres = [dict(lda[doc_bow]) for doc_bow in corpus]\n",
    "    doc_df = pd.DataFrame(docres,dtype=np.float16).fillna(0.001)\n",
    "    doc_df.columns = ['%s_lda_%d'%(name,x) for x in range(n_topic)]\n",
    "    doc_df[target] = x[target]\n",
    "    return doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:49:01.033813Z",
     "start_time": "2019-03-28T10:48:56.830819Z"
    }
   },
   "outputs": [],
   "source": [
    "breed_color_lda = get_lda_feature(data_df, 'Breed1', 'Color_full', 'breed', n_topic=5)\n",
    "breed_breed_lda = get_lda_feature(data_df, 'Breed1', 'Breed2', 'breed_breed', n_topic=5)\n",
    "state_breed_lda = get_lda_feature(data_df, 'State', 'Breed_full', 'State_breed', n_topic=5)\n",
    "state_color_lda = get_lda_feature(data_df, 'State', 'Color_full', 'State_color',n_topic=5)\n",
    "# rescuer_breed_lda = get_lda_feature(data_df, 'RescuerID', 'Breed_full', 'rescuer_breed',n_topic=30)\n",
    "# rescuer_color_lda = get_lda_feature(data_df, 'RescuerID', 'Color_full', 'rescuer_color',n_topic=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:49:01.039205Z",
     "start_time": "2019-03-28T10:49:01.035981Z"
    }
   },
   "outputs": [],
   "source": [
    "# dnn_resnet50_features = pd.read_pickle('image_features_nn.pkl')\n",
    "# svd_resnet50_features = pd.read_pickle('image_features_svd.pkl')\n",
    "# cluster_resnet50_features = pd.read_pickle('image_features_cluster.pkl')\n",
    "\n",
    "# text_features = pd.read_pickle('text_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:39:03.725515Z",
     "start_time": "2019-03-28T12:39:02.827899Z"
    },
    "_uuid": "947ecbc24175147fb4808f7d85068a851d39cd57"
   },
   "outputs": [],
   "source": [
    "# Train merges:\n",
    "data_df_proc = data_df.copy()\n",
    "data_df_proc = data_df_proc.merge(senti_df, how='left', on='PetID')\n",
    "data_df_proc = data_df_proc.merge(meta_df, how='left', on='PetID')\n",
    "data_df_proc = data_df_proc.merge(feat_df, how='left', on='PetID')\n",
    "data_df_proc = data_df_proc.merge(dnn_resnet50_features[['PetID','resnet_%d'%IMG_FEATURE_DIM_NN]], how='left', on='PetID')#\n",
    "data_df_proc = data_df_proc.merge(dnn_dense_features[['PetID','dense_%d'%IMG_FEATURE_DIM_NN]], how='left', on='PetID')#\n",
    "data_df_proc = data_df_proc.merge(dnn_resnet34_features[['PetID','res34_%d'%IMG_FEATURE_DIM_NN2]], how='left', on='PetID')#\n",
    "data_df_proc = data_df_proc.merge(dnn_dense121_features[['PetID','dense121_%d'%IMG_FEATURE_DIM_NN]], how='left', on='PetID')#\n",
    "\n",
    "data_df_proc = data_df_proc.merge(svd_resnet50_features, how='left', on='PetID')\n",
    "data_df_proc = data_df_proc.merge(svd_dense_features, how='left', on='PetID')\n",
    "data_df_proc = data_df_proc.merge(svd_dense121_features, how='left', on='PetID')\n",
    "\n",
    "# data_df_proc = data_df_proc.merge(svd_img_features, how='left', on='PetID')\n",
    "\n",
    "# data_df_proc = data_df_proc.merge(tsne_resnet50_features, how='left', on='PetID')\n",
    "data_df_proc = data_df_proc.merge(cluster_img_features, how='left', on='PetID')\n",
    "# data_df_proc = data_df_proc.merge(cluster_dense_features, how='left', on='PetID')\n",
    "\n",
    "# data_df_proc = data_df_proc.merge(nn_features, how='left', on='PetID') #[['PetID','nn_32']]\n",
    "\n",
    "# lda features\n",
    "data_df_proc = data_df_proc.merge(breed_color_lda, how='left', on='Breed1')\n",
    "data_df_proc = data_df_proc.merge(breed_breed_lda, how='left', on='Breed1')\n",
    "data_df_proc = data_df_proc.merge(state_breed_lda, how='left', on='State')\n",
    "data_df_proc = data_df_proc.merge(state_color_lda, how='left', on='State')\n",
    "\n",
    "# Concatenate with main DF:\n",
    "data_df_proc = data_df_proc.merge(text_features, how='left', on='PetID')\n",
    "# data_df_proc_dummy = pd.get_dummies(data_df_proc, columns=['Breed1','Breed2','Gender','Color1','Color2','Color3',\n",
    "#                                                            'State','Breed_full','Color_full', 'hard_interaction',\n",
    "#                                                            'resnet50_CLUSTER_0'],dummy_na=True)\n",
    "\n",
    "print(data_df_proc.shape)\n",
    "assert data_df_proc.shape[0] == data_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5acc8411e12eb4f741956b1ae608017e10950040"
   },
   "source": [
    "### train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:39:04.363087Z",
     "start_time": "2019-03-28T12:39:04.278274Z"
    },
    "_uuid": "ef5a8fdbe2b786bbc3ffdc797cfa3ca02262b70e"
   },
   "outputs": [],
   "source": [
    "# Split into train and test again:\n",
    "X_train = data_df_proc.iloc[0:train_len]\n",
    "X_test = data_df_proc.iloc[train_len:]\n",
    "# X_train_dummy = data_df_proc_dummy.iloc[0:train_len]\n",
    "# X_test_dummy = data_df_proc_dummy.iloc[train_len:]\n",
    "\n",
    "# Remove missing target column from test:\n",
    "X_test = X_test.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "\n",
    "assert X_train.shape[0] == train.shape[0]\n",
    "assert X_test.shape[0] == test.shape[0]\n",
    "\n",
    "# Check if columns between the two DFs are the same:\n",
    "train_cols = X_train.columns.tolist()\n",
    "train_cols.remove('AdoptionSpeed')\n",
    "\n",
    "test_cols = X_test.columns.tolist()\n",
    "\n",
    "assert np.all(train_cols == test_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a87c86df9fd01e6c1b744a7d21b40d816ca66106"
   },
   "source": [
    "### model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:39:06.952368Z",
     "start_time": "2019-03-28T12:39:06.947565Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Additional parameters:\n",
    "early_stop = 300\n",
    "verbose_eval = 100\n",
    "num_rounds = 10000\n",
    "\n",
    "to_drop_columns = ['PetID', 'Name', 'RescuerID', 'AdoptionSpeed', 'target2', \n",
    "                   'main_breed_Type', 'main_breed_BreedName', 'second_breed_Type', 'second_breed_BreedName',\n",
    "                   'Description', 'sentiment_entities', 'meta_annots_top_desc','meta_desc',\n",
    "                   'Chinese_desc', 'English_desc','BreedName_full','Breed1Name','Breed2Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:49:01.875574Z",
     "start_time": "2019-03-28T10:49:01.868598Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1991)\n",
    "torch.cuda.manual_seed(1991)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:49:08.756538Z",
     "start_time": "2019-03-28T10:49:01.877770Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_cols = ['Type','age_in_year','Breed1','Breed2','Gender','Color1','Color2','Color3','MaturitySize',\n",
    "           'FurLength','Vaccinated','Dewormed','Sterilized','Health','State','Breed_full',\n",
    "           'Color_full', 'hard_interaction','img_CLUSTER_0']\n",
    "fm_data = data_df_proc[fm_cols]\n",
    "fm_values = []\n",
    "for c in fm_cols:\n",
    "    fm_data.loc[:,c] = fm_data[c].fillna(0)\n",
    "    fm_data.loc[:,c] = c+'_'+fm_data[c].astype(str)\n",
    "    fm_values+=fm_data[c].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:49:12.265059Z",
     "start_time": "2019-03-28T10:49:08.759437Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(fm_values)\n",
    "for c in fm_cols:\n",
    "    fm_data.loc[:,c] = lbe.transform(fm_data[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:27:12.429794Z",
     "start_time": "2019-03-28T12:27:12.412301Z"
    }
   },
   "outputs": [],
   "source": [
    "numerical_cols = [x for x in X_train.columns if x not in to_drop_columns+fm_cols+svd_dense_features.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:27:13.109827Z",
     "start_time": "2019-03-28T12:27:13.101736Z"
    }
   },
   "outputs": [],
   "source": [
    "len(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:27:19.723238Z",
     "start_time": "2019-03-28T12:27:19.532725Z"
    }
   },
   "outputs": [],
   "source": [
    "numerical_feats = []\n",
    "for c in numerical_cols:\n",
    "    numerical_feats.append(data_df_proc[c].fillna(0))\n",
    "    \n",
    "# for c in range(1920):\n",
    "#     numerical_feats.append(raw_img_features['resnet50_%d'%c].fillna(0))\n",
    "\n",
    "numerical_feats = np.vstack(numerical_feats).T\n",
    "# numerical_feats = stdscaler.fit_transform(numerical_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:27:25.453594Z",
     "start_time": "2019-03-28T12:27:25.446637Z"
    }
   },
   "outputs": [],
   "source": [
    "numerical_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:49:12.479576Z",
     "start_time": "2019-03-28T10:49:12.465602Z"
    },
    "code_folding": [
     1,
     29
    ]
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 400\n",
    "class PetDesDataset(Dataset):\n",
    "    def __init__(self, sentences, pos, fm_data, numerical_feat,\n",
    "                 mode='train', target=None):\n",
    "        super(PetDesDataset, self).__init__()\n",
    "        self.data = sentences\n",
    "        self.pos = pos\n",
    "        self.target = target\n",
    "        self.mode = mode\n",
    "        self.fm_data = fm_data\n",
    "        self.fm_dim = fm_data.shape[1]\n",
    "        self.numerical_feat = numerical_feat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index not in range(0, self.__len__()):\n",
    "            return self.__getitem__(np.random.randint(0, self.__len__()))\n",
    "        sentence_len = min(MAX_LEN, len(self.data[index]))\n",
    "        sentence = torch.tensor(self.data[index][:sentence_len])\n",
    "        fm_data = self.fm_data[index,:]\n",
    "        pos = torch.tensor(self.pos[index][:sentence_len])\n",
    "\n",
    "        if self.mode != 'test':  # , pos, tag\n",
    "            return sentence, pos, sentence_len, fm_data, self.numerical_feat[index], self.target[index]  # , clf_label\n",
    "        else:\n",
    "            return sentence, pos, sentence_len, fm_data, self.numerical_feat[index]\n",
    "        \n",
    "def nn_collate(batch):\n",
    "    has_label = len(batch[0]) == 6\n",
    "    if has_label:\n",
    "        sentences, poses, lengths, fm_data, numerical_feats, label = zip(*batch)\n",
    "        sentences = nn.utils.rnn.pad_sequence(sentences, batch_first=True).type(torch.LongTensor)\n",
    "        poses = nn.utils.rnn.pad_sequence(poses, batch_first=True).type(torch.LongTensor)\n",
    "        lengths = torch.LongTensor(lengths)\n",
    "        fm_data = torch.LongTensor(fm_data)\n",
    "        numerical_feats = torch.FloatTensor(numerical_feats)\n",
    "        label = torch.FloatTensor(label)\n",
    "        return sentences, poses, lengths, fm_data, numerical_feats, label\n",
    "    else:\n",
    "        sentences, poses, lengths, fm_data, numerical_feats = zip(*batch)\n",
    "        sentences = nn.utils.rnn.pad_sequence(sentences, batch_first=True).type(torch.LongTensor)\n",
    "        poses = nn.utils.rnn.pad_sequence(poses, batch_first=True).type(torch.LongTensor)\n",
    "        lengths = torch.LongTensor(lengths)\n",
    "        fm_data = torch.LongTensor(fm_data)\n",
    "        numerical_feats = torch.FloatTensor(numerical_feats)\n",
    "        return sentences, poses, lengths, fm_data, numerical_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:49:12.490573Z",
     "start_time": "2019-03-28T10:49:12.481684Z"
    },
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def get_mask(sequences_batch, sequences_lengths, cpu=False):\n",
    "    batch_size = sequences_batch.size()[0]\n",
    "    max_length = torch.max(sequences_lengths)\n",
    "    mask = torch.ones(batch_size, max_length, dtype=torch.float)\n",
    "    mask[sequences_batch[:, :max_length] == 0] = 0.0\n",
    "    if cpu:\n",
    "        return mask\n",
    "    else:\n",
    "        return mask.cuda()\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, bias=True, head_num=1, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.head_num = head_num\n",
    "        weight = torch.zeros(feature_dim, self.head_num)\n",
    "        bias = torch.zeros((1, 1, self.head_num))\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        self.b = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, step_dim, feature_dim = x.size()\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim),  # B*L*H\n",
    "            self.weight  # B*H*1\n",
    "        ).view(-1, step_dim, self.head_num)  # B*L*head\n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "        eij = torch.tanh(eij)\n",
    "        if mask is not None:\n",
    "            eij = eij * mask - 99999.9 * (1 - mask)\n",
    "        a = torch.softmax(eij, dim=1)\n",
    "\n",
    "        weighted_input = torch.bmm(x.permute((0,2,1)),\n",
    "                                   a).view(batch_size, -1)\n",
    "        return weighted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:49:12.497825Z",
     "start_time": "2019-03-28T10:49:12.492096Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "class FM(nn.Module):\n",
    "\n",
    "    def __init__(self, max_features, feat_len, embed_size):\n",
    "        super(FM, self).__init__()\n",
    "        self.bias_emb = nn.Embedding(max_features, 1)\n",
    "        self.fm_emb = nn.Embedding(max_features, embed_size)\n",
    "        self.feat_len = feat_len\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        bias = self.bias_emb(x)\n",
    "        bias = torch.sum(bias,1) # N * 1\n",
    "\n",
    "        # second order term\n",
    "        # square of sum\n",
    "        emb = self.fm_emb(x)\n",
    "        sum_feature_emb = torch.sum(emb, 1) # N * k\n",
    "        square_sum_feature_emb = sum_feature_emb*sum_feature_emb\n",
    "\n",
    "        # sum of square\n",
    "        square_feature_emb = emb * emb\n",
    "        sum_square_feature_emb = torch.sum(square_feature_emb, 1) # N * k\n",
    "\n",
    "        second_order = 0.5*(square_sum_feature_emb-sum_square_feature_emb) # N *k\n",
    "        return bias+second_order, emb.view(-1, self.feat_len*self.embed_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:49:12.514549Z",
     "start_time": "2019-03-28T10:49:12.499968Z"
    },
    "code_folding": [
     0,
     23
    ]
   },
   "outputs": [],
   "source": [
    "class FmNlpModel(nn.Module):\n",
    "    def turn_on_embedding(self):\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "    def __init__(self, hidden_size=64, init_embedding=None, head_num=3,\n",
    "                 fm_embed_size=8, fm_feat_len=10, fm_max_feature = 300, numerical_dim = 300,\n",
    "                 nb_word = 40000, nb_pos = 200, pos_emb_size = 10):\n",
    "        super(FmNlpModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(nb_word, 300, padding_idx=0)\n",
    "        self.pos_embedding = nn.Embedding(nb_pos+100, pos_emb_size, padding_idx=0)\n",
    "        \n",
    "        if init_embedding is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(init_embedding))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.fm = FM(fm_max_feature, fm_feat_len, fm_embed_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.attention_gru = Attention(feature_dim=self.hidden_size * 2, head_num=head_num)\n",
    "        self.gru = nn.GRU(embed_size+pos_emb_size, hidden_size, bidirectional=True, batch_first=True) #\n",
    "        self.gru2 = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.dnn = nn.Sequential(\n",
    "            nn.BatchNorm1d(numerical_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(numerical_dim, 256),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.rnn_dnn = nn.Sequential(\n",
    "            nn.BatchNorm1d(fm_embed_size+fm_feat_len*fm_embed_size +2*head_num * hidden_size+128), #\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fm_embed_size+fm_feat_len*fm_embed_size+2*head_num * hidden_size+128, 32),\n",
    "            nn.ELU(inplace=True),\n",
    "        )\n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "#         self.apply(_init_esim_weights)\n",
    "\n",
    "    def forward(self, x, pos_x, len_x, fm_x, numerical_x):\n",
    "        \n",
    "        fm_result, fm_embed = self.fm(fm_x)\n",
    "        \n",
    "        sentence_mask = get_mask(x, len_x)\n",
    "        x = x * sentence_mask.long()\n",
    "        sentence_mask = torch.unsqueeze(sentence_mask, -1)\n",
    "\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_pos_embedding = self.pos_embedding(pos_x)\n",
    "        h_embedding = torch.cat([h_embedding, h_pos_embedding],2)\n",
    "        \n",
    "        h_embedding = self.dropout(h_embedding)\n",
    "        \n",
    "        sorted_seq_lengths, indices = torch.sort(len_x, descending=True)\n",
    "        # \n",
    "        _, desorted_indices = torch.sort(indices, descending=False)\n",
    "        h_embedding = h_embedding[indices]\n",
    "        packed_inputs = nn.utils.rnn.pack_padded_sequence(h_embedding, sorted_seq_lengths, batch_first=True)\n",
    "        \n",
    "        h_gru, _ = self.gru(packed_inputs)\n",
    "        h_gru2, _ = self.gru2(h_gru)  # sentence_mask.expand_as(h_lstm)\n",
    "        \n",
    "        h_gru2, _ = nn.utils.rnn.pad_packed_sequence(h_gru2, batch_first=True)\n",
    "        h_gru2 = h_gru2[desorted_indices]\n",
    "        att_pool_gru = self.attention_gru(h_gru2, sentence_mask)\n",
    "        \n",
    "        numerical_x = self.dnn(numerical_x)\n",
    "\n",
    "        x = torch.cat([att_pool_gru,fm_result,fm_embed,numerical_x],1) # \n",
    "        feat = self.rnn_dnn(x)\n",
    "        out = self.logit(feat)\n",
    "\n",
    "        return out, feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:27:27.539396Z",
     "start_time": "2019-03-28T12:27:27.519473Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_numerical = numerical_feats[0:len(train)]\n",
    "X_test_numerical = numerical_feats[len(train):]\n",
    "\n",
    "X_train_seq = pd.Series(eng_sequences[0:len(train)])\n",
    "X_test_seq = pd.Series(eng_sequences[len(train):])\n",
    "\n",
    "X_train_pos_seq = pd.Series(pos_sequences[0:len(train)])\n",
    "X_test_pos_seq = pd.Series(pos_sequences[len(train):])\n",
    "\n",
    "X_train_fm = fm_data.iloc[0:len(train)].values\n",
    "X_test_fm = fm_data.iloc[len(train):].values\n",
    "\n",
    "Y_train = data_df.iloc[0:len(train)]['AdoptionSpeed'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:37:37.657244Z",
     "start_time": "2019-03-28T12:27:31.590793Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_epochs = 6\n",
    "loss_fn = torch.nn.MSELoss().cuda()\n",
    "oof_train_nlp = np.zeros((X_train.shape[0], 32+1))\n",
    "oof_test_nlp = []\n",
    "\n",
    "test_set = PetDesDataset(X_test_seq.tolist(), X_test_pos_seq.tolist(), X_test_fm, X_test_numerical, mode='test')\n",
    "test_loader = DataLoader(test_set, batch_size=512, shuffle=False, num_workers=1, pin_memory=True,\n",
    "                                collate_fn=nn_collate)\n",
    "qwks = []\n",
    "rmses = []\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(split_index): \n",
    "        \n",
    "    print('fold:', n_fold)\n",
    "    hist = histogram(Y_train[train_idx].astype(int), \n",
    "                     int(np.min(X_train['AdoptionSpeed'])), \n",
    "                     int(np.max(X_train['AdoptionSpeed'])))\n",
    "    tr_cdf = get_cdf(hist)\n",
    "    \n",
    "    training_set = PetDesDataset(X_train_seq[train_idx].tolist(), \n",
    "                                 X_train_pos_seq[train_idx].tolist(),\n",
    "                                 X_train_fm[train_idx], \n",
    "                                 X_train_numerical[train_idx], target = Y_train[train_idx])\n",
    "    \n",
    "    validation_set = PetDesDataset(X_train_seq[valid_idx].tolist(), \n",
    "                                   X_train_pos_seq[valid_idx].tolist(),\n",
    "                                   X_train_fm[valid_idx], \n",
    "                                  X_train_numerical[valid_idx],target = Y_train[valid_idx])\n",
    "    \n",
    "    training_loader = DataLoader(training_set, batch_size=512, shuffle=True, num_workers=1,\n",
    "                                collate_fn=nn_collate)\n",
    "    validation_loader = DataLoader(validation_set, batch_size=512, shuffle=False, num_workers=1,\n",
    "                                collate_fn=nn_collate)\n",
    "    \n",
    "    model = FmNlpModel(hidden_size=48, init_embedding=embedding_matrix, head_num=10, \n",
    "                      fm_embed_size=10, fm_feat_len=X_train_fm.shape[1], fm_max_feature=len(fm_values),\n",
    "                      numerical_dim=X_train_numerical.shape[1],\n",
    "                      nb_word=nb_words, nb_pos=nb_pos, pos_emb_size=10)\n",
    "    model.cuda()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=4, eta_min=0.0001)\n",
    "    \n",
    "    iteration = 0\n",
    "    min_val_loss = 100\n",
    "    since = time.time()\n",
    "    \n",
    "    for epoch in range(train_epochs):       \n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        for sentences, poses, lengths, x_fm, x_numerical, labels in training_loader:\n",
    "            iteration += 1\n",
    "            sentences = sentences.cuda()\n",
    "            poses = poses.cuda()\n",
    "            lengths = lengths.cuda()\n",
    "            x_fm = x_fm.cuda()\n",
    "            x_numerical = x_numerical.cuda()\n",
    "            labels = labels.type(torch.FloatTensor).cuda().view(-1, 1)\n",
    "\n",
    "            pred,_ = model(sentences, poses, lengths, x_fm, x_numerical)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_predicts = []\n",
    "        val_feats = []\n",
    "        with torch.no_grad():\n",
    "            for sentences, poses, lengths, x_fm, x_numerical, labels in validation_loader:\n",
    "                sentences = sentences.cuda()\n",
    "                poses = poses.cuda()\n",
    "                lengths = lengths.cuda()\n",
    "                x_fm = x_fm.cuda()\n",
    "                x_numerical = x_numerical.cuda()\n",
    "                labels = labels.type(torch.FloatTensor).cuda()#.view(-1, 1)\n",
    "                v_pred, v_feat = model(sentences, poses, lengths, x_fm, x_numerical)\n",
    "                val_predicts.append(v_pred.cpu().numpy())\n",
    "                val_feats.append(v_feat.cpu().numpy())\n",
    "\n",
    "        val_predicts = np.concatenate(val_predicts)\n",
    "        val_feats = np.vstack(val_feats)\n",
    "        val_loss = rmse(Y_train[valid_idx], val_predicts)\n",
    "        if val_loss<min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            oof_train_nlp[valid_idx,:] = np.hstack([val_feats, val_predicts])\n",
    "            test_feats = []\n",
    "            test_preds = []\n",
    "            with torch.no_grad():\n",
    "                for sentences, poses, lengths, x_fm, x_numerical in test_loader:\n",
    "                    sentences = sentences.cuda()\n",
    "                    poses = poses.cuda()\n",
    "                    lengths = lengths.cuda()\n",
    "                    x_fm = x_fm.cuda()\n",
    "                    x_numerical = x_numerical.cuda()\n",
    "                    v_pred, feat = model(sentences, poses, lengths, x_fm, x_numerical)\n",
    "                    test_preds.append(v_pred.cpu().numpy())\n",
    "                    test_feats.append(feat.cpu().numpy())\n",
    "            test_feats = np.hstack([np.vstack(test_feats), np.concatenate(test_preds)])\n",
    "#             pred_test_y_k = getTestScore2(val_predicts.flatten(), tr_cdf)\n",
    "#             qwk = quadratic_weighted_kappa(Y_train[valid_idx], pred_test_y_k)\n",
    "#             print(epoch, \"val loss:\", val_loss, \"val QWK_2 = \", qwk, \"elapsed time:\", time.time()-since)\n",
    "    oof_test_nlp.append(test_feats)\n",
    "    del model\n",
    "    del training_set\n",
    "    del validation_set \n",
    "    del sentences\n",
    "    del lengths\n",
    "    del x_fm\n",
    "    del x_numerical\n",
    "    del poses\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "#     qwks.append(qwk)\n",
    "#     rmses.append(min_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:37:53.826878Z",
     "start_time": "2019-03-28T12:37:53.815783Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('overall rmse: %.5f'%rmse(oof_train_nlp[:,-1], X_train['AdoptionSpeed']))\n",
    "# print('mean rmse =', np.mean(rmses), 'rmse std =', np.std(rmses))\n",
    "# print('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T10:59:05.988345Z",
     "start_time": "2019-03-28T10:59:05.985770Z"
    }
   },
   "outputs": [],
   "source": [
    "# del model\n",
    "# del training_set\n",
    "# del validation_set \n",
    "# del sentences\n",
    "# del lengths\n",
    "# del x_fm\n",
    "# del x_numerical\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:37:57.260977Z",
     "start_time": "2019-03-28T12:37:57.252400Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_test_nlp = np.mean(oof_test_nlp, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:39:13.041471Z",
     "start_time": "2019-03-28T12:39:13.028104Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "features = [x for x in X_train.columns if x not in to_drop_columns+svd_resnet50_features.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:39:15.162681Z",
     "start_time": "2019-03-28T12:39:15.157216Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:48:25.373507Z",
     "start_time": "2019-03-28T12:39:26.845645Z"
    },
    "_uuid": "58ab4de93b0345c8c251e81f7de173de375a7953",
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.01,\n",
    "          'subsample': 0.85,\n",
    "          'feature_fraction': 0.7,\n",
    "          'lambda_l1':0.01,\n",
    "          'verbosity': -1,\n",
    "         }\n",
    "\n",
    "oof_train_lgb = np.zeros((X_train.shape[0]))\n",
    "oof_test_lgb = []\n",
    "qwks = []\n",
    "rmses = []\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(split_index):\n",
    "    since = time.time()\n",
    "    X_tr = X_train.iloc[train_idx]\n",
    "    X_val = X_train.iloc[valid_idx]\n",
    "\n",
    "    y_tr = X_tr['AdoptionSpeed'].values    \n",
    "    y_val = X_val['AdoptionSpeed'].values\n",
    "        \n",
    "    d_train = lgb.Dataset(X_tr[features], label=y_tr,\n",
    "#                          categorical_feature=['Breed1','Color1','Breed2','State','Breed_full','Color_full']\n",
    "                         )\n",
    "    d_valid = lgb.Dataset(X_val[features], label=y_val, reference=d_train)\n",
    "    watchlist = [d_valid]\n",
    "    \n",
    "    print('training LGB:')\n",
    "    lgb_model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=500,\n",
    "                      early_stopping_rounds=100,\n",
    "                     )\n",
    "    \n",
    "    val_pred = lgb_model.predict(X_val[features])\n",
    "    test_pred = lgb_model.predict(X_test[features])\n",
    "    train_pred = lgb_model.predict(X_tr[features])\n",
    "    \n",
    "    oof_train_lgb[valid_idx] = val_pred\n",
    "    oof_test_lgb.append(test_pred)\n",
    "               \n",
    "#     hist = histogram(X_tr['AdoptionSpeed'].astype(int), \n",
    "#                      int(np.min(X_train['AdoptionSpeed'])), \n",
    "#                      int(np.max(X_train['AdoptionSpeed'])))\n",
    "#     tr_cdf = get_cdf(hist)\n",
    "#     _, cutoff = getScore(train_pred, tr_cdf, True)\n",
    "#     pred_test_y_k = getTestScore(val_pred, cutoff)\n",
    "#     qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n",
    "#     print(\"QWK_1 = \", qwk)\n",
    "    \n",
    "#     pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n",
    "#     qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n",
    "#     qwks.append(qwk)\n",
    "#     rmses.append(rmse(X_val['AdoptionSpeed'].values, val_pred))\n",
    "#     print(\"QWK_2 = \", qwk,'elapsed time:',time.time()-since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:48:55.156009Z",
     "start_time": "2019-03-28T12:48:55.145328Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('overall rmse: %.5f'%rmse(oof_train_lgb, X_train['AdoptionSpeed']))\n",
    "print('mean rmse =', np.mean(rmses), 'rmse std =', np.std(rmses))\n",
    "print('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:49:00.015686Z",
     "start_time": "2019-03-28T12:48:58.271739Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# optR = OptimizedRounder()\n",
    "# optR.fit(oof_train_lgb, X_train['AdoptionSpeed'].values)\n",
    "# coefficients = optR.coefficients()\n",
    "# pred_test_y_k = optR.predict(oof_train_lgb, coefficients)\n",
    "# qwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, pred_test_y_k)\n",
    "# print(\"QWK = \", qwk)\n",
    "# hist = histogram(X_train['AdoptionSpeed'].astype(int), \n",
    "#                      int(np.min(X_train['AdoptionSpeed'])), \n",
    "#                      int(np.max(X_train['AdoptionSpeed'])))\n",
    "# tr_cdf = get_cdf(hist)\n",
    "# pred_test_y_k = getTestScore2(oof_train_lgb, tr_cdf)\n",
    "# qwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, pred_test_y_k)\n",
    "# print(\"QWK = \", qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T13:00:09.622149Z",
     "start_time": "2019-03-28T12:50:06.411485Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# features = [x for x in X_train.columns if x not in to_drop_columns+svd_dense_features.columns.tolist()]\n",
    "# params = {'application': 'regression',\n",
    "#           'boosting': 'gbdt',\n",
    "#           'metric': 'rmse',\n",
    "#           'num_leaves': 70,\n",
    "#           'max_depth': 9,\n",
    "#           'learning_rate': 0.005,\n",
    "#           'subsample': 0.85,\n",
    "#           'feature_fraction': 0.7,\n",
    "# #           'lambda_l1':0.01,\n",
    "#           'verbosity': -1,\n",
    "#          }\n",
    "\n",
    "# oof_train_lgb2 = np.zeros((X_train.shape[0]))\n",
    "# oof_test_lgb2 = []\n",
    "# qwks = []\n",
    "# rmses = []\n",
    "\n",
    "# for n_fold, (train_idx, valid_idx) in enumerate(split_index):\n",
    "#     since = time.time()\n",
    "#     X_tr = X_train.iloc[train_idx]\n",
    "#     X_val = X_train.iloc[valid_idx]\n",
    "\n",
    "#     y_tr = X_tr['AdoptionSpeed'].values    #apply(target_transform).\n",
    "#     y_val = X_val['AdoptionSpeed'].values #apply(target_transform)\n",
    "        \n",
    "#     d_train = lgb.Dataset(X_tr[features], label=y_tr,\n",
    "#                          categorical_feature=['Type','age_in_year','Breed1','Breed2','Gender','Color1','Color2','Color3','MaturitySize',\n",
    "#            'FurLength','Vaccinated','Dewormed','Sterilized','Health','State','Breed_full',\n",
    "#            'Color_full', 'hard_interaction','img_CLUSTER_0']\n",
    "#                          )\n",
    "#     d_valid = lgb.Dataset(X_val[features], label=y_val, reference=d_train)\n",
    "#     watchlist = [d_valid]\n",
    "    \n",
    "#     print('training LGB:')\n",
    "#     lgb_model = lgb.train(params,\n",
    "#                       train_set=d_train,\n",
    "#                       num_boost_round=num_rounds,\n",
    "#                       valid_sets=watchlist,\n",
    "#                       verbose_eval=500,\n",
    "#                       early_stopping_rounds=early_stop,\n",
    "#                      )\n",
    "    \n",
    "#     val_pred = lgb_model.predict(X_val[features])\n",
    "#     test_pred = lgb_model.predict(X_test[features])\n",
    "    \n",
    "#     oof_train_lgb2[valid_idx] = val_pred\n",
    "#     oof_test_lgb2.append(test_pred)\n",
    "               \n",
    "#     hist = histogram(X_tr['AdoptionSpeed'].astype(int), \n",
    "#                      int(np.min(X_train['AdoptionSpeed'])), \n",
    "#                      int(np.max(X_train['AdoptionSpeed'])))\n",
    "#     tr_cdf = get_cdf(hist)\n",
    "    \n",
    "#     pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n",
    "#     qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n",
    "#     qwks.append(qwk)\n",
    "#     rmses.append(rmse(X_val['AdoptionSpeed'].values, val_pred))\n",
    "#     print(\"QWK_2 = \", qwk,'elapsed time:',time.time()-since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T13:00:21.185538Z",
     "start_time": "2019-03-28T13:00:21.173855Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print('overall rmse: %.5f'%rmse(oof_train_lgb2, X_train['AdoptionSpeed']))\n",
    "# print('mean rmse =', np.mean(rmses), 'rmse std =', np.std(rmses))\n",
    "# print('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T11:11:37.670462Z",
     "start_time": "2019-03-28T11:11:37.632471Z"
    }
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T11:54:59.084251Z",
     "start_time": "2019-03-28T11:54:59.075758Z"
    }
   },
   "outputs": [],
   "source": [
    "features = [x for x in X_train.columns if x not in to_drop_columns+svd_dense121_features.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T11:54:59.096140Z",
     "start_time": "2019-03-28T11:54:59.090713Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_index = []\n",
    "for idx, c in enumerate(features):\n",
    "    if c in ['Type','Breed1','Breed2','Gender','Color1','Color2','Color3','State','Breed_full',\n",
    "           'Color_full', 'hard_interaction','img_CLUSTER_0']:\n",
    "        cat_index.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:05:51.089765Z",
     "start_time": "2019-03-28T11:54:59.098659Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_train_cat = np.zeros((X_train.shape[0]))\n",
    "oof_test_cat = []\n",
    "qwks = []\n",
    "rmses = []\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(split_index):\n",
    "    since = time.time()\n",
    "    X_tr = X_train.iloc[train_idx]\n",
    "    X_val = X_train.iloc[valid_idx]\n",
    "\n",
    "    y_tr = X_tr['AdoptionSpeed'].values    #apply(target_transform).\n",
    "    y_val = X_val['AdoptionSpeed'].values #apply(target_transform)\n",
    "        \n",
    "    \n",
    "    eval_dataset = Pool(X_val[features].values,\n",
    "                    y_val,\n",
    "                   cat_index)\n",
    "    print('training Catboost:')\n",
    "    model = CatBoostRegressor(learning_rate=0.01,  depth=8, task_type = \"GPU\", l2_leaf_reg=1)\n",
    "    model.fit(X_tr[features].values,\n",
    "              y_tr,\n",
    "              eval_set=eval_dataset,\n",
    "              cat_features= cat_index,\n",
    "              verbose=False)\n",
    "    \n",
    "    val_pred = model.predict(eval_dataset)\n",
    "    test_pred = model.predict(X_test[features])\n",
    "    \n",
    "    oof_train_cat[valid_idx] = val_pred\n",
    "    oof_test_cat.append(test_pred)\n",
    "               \n",
    "#     hist = histogram(X_tr['AdoptionSpeed'].astype(int), \n",
    "#                      int(np.min(X_train['AdoptionSpeed'])), \n",
    "#                      int(np.max(X_train['AdoptionSpeed'])))\n",
    "#     tr_cdf = get_cdf(hist)\n",
    "    \n",
    "#     pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n",
    "#     qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n",
    "#     qwks.append(qwk)\n",
    "#     rmses.append(rmse(X_val['AdoptionSpeed'].values, val_pred))\n",
    "#     print('rmse=',rmses[-1],\"QWK_2 = \", qwk,'elapsed time:',time.time()-since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:05:51.098091Z",
     "start_time": "2019-03-28T12:05:51.092107Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('overall rmse: %.5f'%rmse(oof_train_cat, X_train['AdoptionSpeed']))\n",
    "# print('mean rmse =', np.mean(rmses), 'rmse std =', np.std(rmses))\n",
    "# print('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in X_train.columns if x not in to_drop_columns+svd_dense_features.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:05:51.111993Z",
     "start_time": "2019-03-28T12:05:51.108865Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:05:51.117386Z",
     "start_time": "2019-03-28T12:05:51.113565Z"
    }
   },
   "outputs": [],
   "source": [
    "len(xgb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:13:23.663920Z",
     "start_time": "2019-03-28T12:05:51.119215Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'reg:linear', #huber\n",
    "        'eval_metric':'rmse',\n",
    "        'eta': 0.01,\n",
    "        'tree_method':'gpu_hist',\n",
    "        'max_depth': 9,  \n",
    "        'subsample': 0.85,  \n",
    "        'colsample_bytree': 0.7,     \n",
    "        'alpha': 0.01,  \n",
    "    } \n",
    "\n",
    "oof_train_xgb = np.zeros((X_train.shape[0]))\n",
    "oof_test_xgb = []\n",
    "qwks = []\n",
    "\n",
    "i = 0\n",
    "test_set = xgb.DMatrix(X_test[xgb_features])\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(split_index):  \n",
    "    X_tr = X_train.iloc[train_idx]\n",
    "    X_val = X_train.iloc[valid_idx]\n",
    "    \n",
    "    y_tr = X_tr['AdoptionSpeed'].values    \n",
    "    y_val = X_val['AdoptionSpeed'].values\n",
    "        \n",
    "    d_train = xgb.DMatrix(X_tr[xgb_features], y_tr)\n",
    "    d_valid = xgb.DMatrix(X_val[xgb_features], y_val)\n",
    "    watchlist = [d_valid]\n",
    "    since = time.time()\n",
    "    print('training XGB:')\n",
    "    model = xgb.train(params, d_train, num_boost_round = 10000, evals=[(d_valid,'val')],\n",
    "                     early_stopping_rounds=100, #feval=xgb_eval_kappa,\n",
    "                     verbose_eval=500)\n",
    "    \n",
    "    val_pred = model.predict(d_valid)\n",
    "    test_pred = model.predict(test_set)\n",
    "    \n",
    "    oof_train_xgb[valid_idx] = val_pred\n",
    "    oof_test_xgb.append(test_pred)\n",
    "    \n",
    "#     hist = histogram(X_tr['AdoptionSpeed'].astype(int), \n",
    "#                      int(np.min(X_train['AdoptionSpeed'])), \n",
    "#                      int(np.max(X_train['AdoptionSpeed'])))\n",
    "#     tr_cdf = get_cdf(hist)\n",
    "    \n",
    "#     pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n",
    "#     qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n",
    "#     qwks.append(qwk)\n",
    "#     print(\"QWK_2 = \", qwk,'elapsed time:',time.time()-since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:13:23.683155Z",
     "start_time": "2019-03-28T12:13:23.670205Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('overall rmse: %.5f'%rmse(oof_train_xgb, X_train['AdoptionSpeed']))\n",
    "# print('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T03:13:41.676179Z",
     "start_time": "2019-03-24T03:13:41.658914Z"
    }
   },
   "source": [
    "## stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef([np.mean(oof_test_lgb,axis=0), \n",
    "             #np.mean(oof_test_lgb2,axis=0),\n",
    "             np.mean(oof_test_cat,axis=0),\n",
    "             np.mean(oof_test_xgb,axis=0),\n",
    "             oof_test_nlp[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T13:10:30.008560Z",
     "start_time": "2019-03-28T13:10:29.735165Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_stacking = np.vstack([oof_train_lgb, \n",
    "#                               oof_train_lgb2,\n",
    "                              oof_train_xgb, \n",
    "                              oof_train_cat, \n",
    "                              oof_train_nlp[:,-1]]).T\n",
    "X_test_stacking = np.vstack([np.mean(oof_test_lgb, axis=0),\n",
    "#                              np.mean(oof_test_lgb2, axis=0),\n",
    "                             np.mean(oof_test_xgb, axis=0),\n",
    "                             np.mean(oof_test_cat,axis=0),\n",
    "                             oof_test_nlp[:,-1]]).T\n",
    "\n",
    "stacking_train = np.zeros((X_train.shape[0]))\n",
    "stacking_test = []\n",
    "rmses, qwks = [], []\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(split_index):\n",
    "    \n",
    "    X_tr = X_train_stacking[train_idx]\n",
    "    X_val = X_train_stacking[valid_idx]\n",
    "    \n",
    "    y_tr = X_train.iloc[train_idx]['AdoptionSpeed'].values    \n",
    "    y_val = X_train.iloc[valid_idx]['AdoptionSpeed'].values\n",
    "        \n",
    "    since = time.time()\n",
    "    \n",
    "    print('training Ridge:')\n",
    "    model = Ridge(alpha=1)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    print(model.coef_)\n",
    "    \n",
    "    val_pred = model.predict(X_val)\n",
    "    test_pred = model.predict(X_test_stacking)\n",
    "    \n",
    "    stacking_train[valid_idx] = val_pred\n",
    "    stacking_test.append(test_pred)\n",
    "    loss = rmse(Y_train[valid_idx], val_pred)\n",
    "    hist = histogram(y_tr.astype(int), \n",
    "                     int(np.min(X_train['AdoptionSpeed'])), \n",
    "                     int(np.max(X_train['AdoptionSpeed'])))\n",
    "    tr_cdf = get_cdf(hist)\n",
    "    \n",
    "    pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n",
    "    qwk = quadratic_weighted_kappa(y_val, pred_test_y_k)\n",
    "    qwks.append(qwk)\n",
    "    rmses.append(loss)\n",
    "    print(\"RMSE=\",loss, \"QWK_2 = \", qwk,'elapsed time:',time.time()-since)\n",
    "stacking_test = np.mean(stacking_test, axis=0)\n",
    "print('mean rmse:',np.mean(rmses), 'rmse std:', np.std(rmses))\n",
    "print('mean qwk:', np.mean(qwks), 'qwk std:', np.std(qwks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T13:10:31.915321Z",
     "start_time": "2019-03-28T13:10:31.872046Z"
    },
    "_uuid": "476da82fb0a09d8ae23afb03859e05b70a87b44d"
   },
   "outputs": [],
   "source": [
    "# Compute QWK based on OOF train predictions:\n",
    "hist = histogram(X_train['AdoptionSpeed'].astype(int), \n",
    "                 int(np.min(X_train['AdoptionSpeed'])), \n",
    "                 int(np.max(X_train['AdoptionSpeed'])))\n",
    "tr_cdf = get_cdf(hist)\n",
    "train_predictions = getTestScore2(stacking_train, tr_cdf)\n",
    "test_predictions = getTestScore2(stacking_test, tr_cdf)\n",
    "qwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, train_predictions)\n",
    "print(\"QWK = \", qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T11:32:53.348393Z",
     "start_time": "2019-03-28T11:32:53.328715Z"
    },
    "_uuid": "03ded9b8f81d756c3dc128d0d9a517d36a30c073",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution inspection of original target and predicted train and test:\n",
    "\n",
    "# print(\"True Distribution:\")\n",
    "# print(pd.value_counts(X_train['AdoptionSpeed'], normalize=True).sort_index())\n",
    "# print(\"\\nTrain Predicted Distribution:\")\n",
    "# print(pd.value_counts(train_predictions, normalize=True).sort_index())\n",
    "# print(\"\\nTest Predicted Distribution:\")\n",
    "# print(pd.value_counts(test_predictions, normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T11:31:44.794298Z",
     "start_time": "2019-03-28T11:31:44.773758Z"
    },
    "_uuid": "49f62f5776c7f47f561541a928231d750d09d3ce"
   },
   "outputs": [],
   "source": [
    "# Generate submission:\n",
    "\n",
    "submission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions.astype(np.int32)})\n",
    "# submission.head()\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
